{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "celltoolbar": "Edit Metadata",
    "kernelspec": {
      "display_name": "R",
      "language": "R",
      "name": "ir"
    },
    "language_info": {
      "codemirror_mode": "r",
      "file_extension": ".r",
      "mimetype": "text/x-r-source",
      "name": "R",
      "pygments_lexer": "r",
      "version": "3.6.3"
    },
    "colab": {
      "name": "Report_Incardona_Biagio.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/biagio-incardona/biagio-incardona.github.io/blob/master/Report_Incardona_Biagio2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JOdniV63j8gy"
      },
      "source": [
        "# <center>Health Insurance Cross Sell Prediction Report</center>\n",
        "\n",
        "![intro](https://gitlab.com/gerasia/temp/-/raw/master/new_intro.jpg)\n",
        "<div style=\"text-align: right\">  <b>MADE BY</b>: Incardona Biagio</div>\n",
        "<div style=\"text-align: right\">  <b>UNIVERSITY ID (MATRICOLA)</b>: 1000023753 </div>\n",
        "\n",
        "## Context\n",
        "\n",
        "Object to study of this report is the ***Health Insurance Cross Sell Prediction*** dataset, freely available on [Kaggle](https://www.kaggle.com/anmolkumar/health-insurance-cross-sell-prediction?select=train.csv).\n",
        "\n",
        "The dataset consists in a single *csv* file containing informations about some potential customers of an Healt Insurance company. \n",
        "The Insurance company has provided Health Insurance to its customers now they need to identify if there are some   policyholders (customers) that will also be interested in Vehicle Insurance provided by the company.\n",
        "\n",
        "## Content\n",
        "\n",
        "The dataset has approximately 400.000 rows and 12 columns. These columns contains personal informations about the potential customer like age, gender, region code, but also informations about his car (if the user has one) and his life as a driver (if the user can drive). Since the dataset is labeled it contains also a *Response* variable, which tell us if the user has accepted the new contract or not. Sice we won't do a classification analysis, we can delete this column or use it just for validating the clusters we will find (if any).\n",
        "\n",
        "## Disclaimer\n",
        "\n",
        "Since our machine is really powerless, using the entire dataset for some operations will takes even more than 4 hours of computing (real test, not finished the computation) or even a crash of the system, we will apply a random sampling of the dataset in order to reduce the number of rows from about 400.000 to 5.000 units and we will do all the analysis and discussions on the reduced dataset.\n",
        "\n",
        "I know that in this way we are losing a lot of important informations and there are a lot of probabilities that the row reduction operation will be an high negative bias on the analysis, but this was the only way to compute some operations in a reasonable time! For example it's impossible to compute a $400.000 \\times 400.000$ distance matrix \n",
        "\n",
        "## Packages\n",
        "\n",
        "The following code cell will be used to import all the packages needed to this report"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NwRhZ1C-j8gy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "00113a66-5dd5-436b-b418-9286a6150d09"
      },
      "source": [
        "# shut down all the warnings\n",
        "options(warn = -1)\n",
        "\n",
        "# install libraries\n",
        "lib.names <- c(\"devtools\",\"Hmisc\", \"mclust\", \"fpc\", \"data.table\",\n",
        "                \"NbClust\",\"corrplot\",\"spgs\", \"factoextra\", \"ggridges\",\n",
        "                \"tidyr\", \"clustertend\", \"vcd\", \"ltm\",\"StatMatch\",\n",
        "                \"gamlss\", \"gamlss.mx\", \"boot\",\"ggfortify\")\n",
        "\n",
        "install.packages(lib.names)\n",
        "\n",
        "# load libraries\n",
        "library(ggplot2)\n",
        "library(ggfortify)\n",
        "library(boot)\n",
        "library(dplyr)\n",
        "library(gamlss)\n",
        "library(gamlss.mx)\n",
        "library(Hmisc)\n",
        "library(data.table)\n",
        "library(corrplot)\n",
        "library(factoextra)\n",
        "library(ggridges)\n",
        "library(tidyr)\n",
        "library(clustertend)\n",
        "library(vcd)\n",
        "library(ltm)\n",
        "library(StatMatch)\n",
        "library(cluster)\n",
        "library(devtools)\n",
        "library(fpc)\n",
        "library(mclust)\n",
        "install_github(\"biagio-incardona/myClValid\")\n",
        "library(myClValid)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Installing packages into ‘/usr/local/lib/R/site-library’\n",
            "(as ‘lib’ is unspecified)\n",
            "\n",
            "also installing the dependencies ‘matrixStats’, ‘RcppArmadillo’, ‘zip’, ‘SparseM’, ‘MatrixModels’, ‘conquer’, ‘sp’, ‘openxlsx’, ‘nloptr’, ‘statmod’, ‘RcppEigen’, ‘carData’, ‘pbkrtest’, ‘quantreg’, ‘maptools’, ‘rio’, ‘lme4’, ‘png’, ‘jpeg’, ‘checkmate’, ‘modeltools’, ‘DEoptimR’, ‘car’, ‘ellipse’, ‘flashClust’, ‘leaps’, ‘scatterplot3d’, ‘ggsci’, ‘cowplot’, ‘ggsignif’, ‘polynom’, ‘rstatix’, ‘zoo’, ‘mvtnorm’, ‘expm’, ‘minqa’, ‘numDeriv’, ‘mitools’, ‘Formula’, ‘latticeExtra’, ‘gridExtra’, ‘htmlTable’, ‘viridis’, ‘flexmix’, ‘prabclus’, ‘diptest’, ‘robustbase’, ‘kernlab’, ‘abind’, ‘dendextend’, ‘FactoMineR’, ‘ggpubr’, ‘reshape2’, ‘ggrepel’, ‘plyr’, ‘lmtest’, ‘msm’, ‘polycor’, ‘proxy’, ‘survey’, ‘lpSolve’, ‘gamlss.data’, ‘gamlss.dist’\n",
            "\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y6-VIipbj8gy"
      },
      "source": [
        "## 1. Preliminar analysis and data cleaning\n",
        "\n",
        "Let's start importing the file into the system."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mhzQR96Mj8gy"
      },
      "source": [
        "# Loading the dataset into the 'insurance.data' variable\n",
        "insurance.data <- read.csv('Healt_Insurance.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VjeuT4-fqUX-"
      },
      "source": [
        "First of all let's define some useful functions for univariate analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a3Mt-C2LqTsT"
      },
      "source": [
        "# generate distribution of the following families\n",
        "# GA, BE, EXP, IGAMMA, IG, JSU, GG, GU, LO, LOGNO, WEI\n",
        "getDens <- function(seqs, family,  i, mu.hats, sigma.hats = NULL, nu.hats = NULL, tau.hats = NULL){\n",
        "  dens <- NULL\n",
        "  switch(family,\n",
        "    GA = {\n",
        "      dens <- dGA(seqs, mu = mu.hats[i], sigma = sigma.hats[i])\n",
        "    },\n",
        "    BE = {\n",
        "      dens <- dBE(seqs, mu = mu.hats[i], sigma = sigma.hats[i])\n",
        "    },\n",
        "    EXP = {\n",
        "      dens <- dEXP(seqs, mu=mu.hats[i])\n",
        "    },\n",
        "    IGAMMA ={\n",
        "      dens <- dIGAMMA(seqs, mu=mu.hats[i], sigma = sigma.hats[i])\n",
        "    },\n",
        "    IG = {\n",
        "      dens <- dIG(seqs, mu=mu.hats[i], sigma = sigma.hats[i])\n",
        "    },\n",
        "    JSU = {\n",
        "      dens <- dJSU(seqs, mu = mu.hats[i], sigma = sigma.hats[i], \n",
        "                   nu = nu.hats[i], tau = tau.hats[i])\n",
        "    },\n",
        "    GG = {\n",
        "      dens <- dGG(seqs, mu=mu.hats[i], sigma = sigma.hats[i], nu = nu.hats[i])\n",
        "    },\n",
        "    GU = {\n",
        "      dens <- dGU(seqs, mu=mu.hats[i], sigma = sigma.hats[i])\n",
        "    },\n",
        "    LO = {\n",
        "      dens <- dLO(seqs, mu = mu.hats[i], sigma = sigma.hats[i])\n",
        "    },\n",
        "    LOGNO = {\n",
        "      dens <- dLOGNO(seqs, mu = mu.hats[i], sigma = sigma.hats[i])\n",
        "    },\n",
        "    WEI = {\n",
        "      dens <- dWEI(seqs, mu = mu.hats[i], sigma = sigma.hats[i])\n",
        "    })\n",
        "  return(dens)\n",
        "}\n",
        "\n",
        "# plot gamlss.mx for any number of distributions with family in:\n",
        "# GA, BE, EXP, IGAMMA, IG, JSU, GG, GU, LO, LOGNO, WEI\n",
        "plot.gamlss <- function(var, family, fit, K, mu.hats, sigma.hats = NULL, nu.hats = NULL, tau.hats = NULL){\n",
        "  hist(var, breaks = 70, freq = F, main = paste(\"histogram of: \", family))\n",
        "  seqs <- seq(min(var),max(var),length=length(var))\n",
        "  for(i in 1:K){\n",
        "    dens <- getDens(seqs, family, i, mu.hats, sigma.hats, nu.hats, tau.hats)\n",
        "    lines(seqs, fit[[\"prob\"]][i] * dens, lty=2, lwd=3, col=i+1)\n",
        "  }\n",
        "  dens <- getDens(seqs, family, 1, mu.hats, sigma.hats, nu.hats, tau.hats)\n",
        "  all <- fit[[\"prob\"]][1] * dens\n",
        "\n",
        "  for(i in 2:K){\n",
        "    dens <- getDens(seqs, family, i, mu.hats, sigma.hats, nu.hats, tau.hats)\n",
        "    all <- all + fit[[\"prob\"]][i] * dens\n",
        "  }\n",
        "  lines(seqs, all, lty = 1, lwd = 3, col = 1)\n",
        "}\n",
        "\n",
        "# get mu from any mixed gamlss of any family\n",
        "get.mu.hats <- function(fit, K){\n",
        "  mu.hats <- c()\n",
        "  for(i in 1:K){\n",
        "    switch(fit[[\"models\"]][[i]][[\"mu.link\"]],\n",
        "      \n",
        "      log = mu.hats[i] <- exp(fit[[\"models\"]][[i]][[\"mu.coefficients\"]]),\n",
        "      \n",
        "      logit = mu.hats[i] <- inv.logit(fit[[\"models\"]][[i]][[\"mu.coefficients\"]]),\n",
        "      \n",
        "      identity = mu.hats[i] <- fit[[\"models\"]][[i]][[\"mu.coefficients\"]]\n",
        "    )\n",
        "  }\n",
        "  return(mu.hats)\n",
        "}\n",
        "\n",
        "# get sigma from any mixed gamlss of any family\n",
        "get.sigma.hats <- function(fit, K){\n",
        "  sigma.hats <- c()\n",
        "  for(i in 1:K){\n",
        "    switch(fit[[\"models\"]][[i]][[\"sigma.link\"]],\n",
        "\n",
        "      log = sigma.hats[i] <- exp(fit[[\"models\"]][[i]][[\"sigma.coefficients\"]]),\n",
        "    \n",
        "      logit = sigma.hats[i] <- inv.logit(fit[[\"models\"]][[i]][[\"sigma.coefficients\"]]),\n",
        "    \n",
        "      identity = sigma.hats[i] <- fit[[\"models\"]][[i]][[\"sigma.coefficients\"]]\n",
        "    )\n",
        "  }\n",
        "  return(sigma.hats)\n",
        "}\n",
        "\n",
        "# get nu from any mixed gamlss of any family\n",
        "get.nu.hats <- function(fit, K){\n",
        "  nu.hats <- c()\n",
        "  for(i in 1:K){\n",
        "    switch(fit[[\"models\"]][[i]][[\"nu.link\"]],\n",
        "    \n",
        "      log = nu.hats[i] <- exp(fit[[\"models\"]][[i]][[\"nu.coefficients\"]]),\n",
        "\n",
        "      logit = nu.hats[i] <- inv.logit(fit[[\"models\"]][[i]][[\"nu.coefficients\"]]),\n",
        "\n",
        "      identity = nu.hats[i] <- fit[[\"models\"]][[i]][[\"nu.coefficients\"]]\n",
        "    )\n",
        "  }\n",
        "  return(nu.hats)\n",
        "}\n",
        "\n",
        "# get tau from any mixed gamlss of any family\n",
        "get.tau.hats <- function(fit, K){\n",
        "  tau.hats <- c()\n",
        "  for(i in 1:K){\n",
        "    switch(fit[[\"models\"]][[i]][[\"tau.link\"]],\n",
        "      log = tau.hats[i] <- exp(fit[[\"models\"]][[i]][[\"tau.coefficients\"]]),\n",
        "\n",
        "      logit = tau.hats[i] <- inv.logit(fit[[\"models\"]][[i]][[\"tau.coefficients\"]]),\n",
        "\n",
        "      identity = tau.hats[i] <- fit[[\"models\"]][[i]][[\"tau.coefficients\"]]\n",
        "    )\n",
        "  }\n",
        "  return(tau.hats)\n",
        "}\n",
        "\n",
        "showBest <- function(df){\n",
        "  max.AIC <- rownames(df[which.max(df$AIC),])\n",
        "  max.logLik <- rownames(df[which.max(df$logLik),])\n",
        "  if (max.AIC == max.logLik){\n",
        "    print(paste(\"best family according the majority rule:\", df[max.AIC,1]))\n",
        "  }\n",
        "  else{\n",
        "    print(paste(\"best family for AIC: \", max.AIC))\n",
        "    print(paste(\"best family for logLikelihood: \", max.logLik))\n",
        "    print(\"best families according to AIC\")\n",
        "    print(head(df %>% arrange(desc(AIC))))\n",
        "\n",
        "    print(\"best families according to logLik\")\n",
        "    print(head(df %>% arrange(desc(logLik))))\n",
        "  }\n",
        "  \n",
        "}\n",
        "\n",
        "# test 10 gamlss.mx different families\n",
        "test.all.gamlss.mx <- function(var, k){\n",
        "\n",
        "  families <- c()\n",
        "  AICs <- c()\n",
        "  logLiks <- c()\n",
        "\n",
        "  tryCatch(expr = {\n",
        "    fit.GA <- gamlssMXfits(n=1,var~1, family=GA, K=k, data = NULL)\n",
        "    families <- append(families, \"GA\")\n",
        "    AICs <- append(AICs, -AIC(fit.GA))\n",
        "    logLiks <- append(logLiks, logLik(fit.GA))\n",
        "  }, error = function(e){\n",
        "    print(\"Error in GA, passing over...\")\n",
        "  })\n",
        "\n",
        "  tryCatch(expr = {\n",
        "    fit.GU <- gamlssMXfits(n=1,var~1, family=GU, K=k, data = NULL)\n",
        "    families <- append(families, \"GU\")\n",
        "    AICs <- append(AICs, -AIC(fit.GU))\n",
        "    logLiks <- append(logLiks, logLik(fit.GU))\n",
        "  }, error = function(e){\n",
        "    print(\"Error in GU, passing over...\")\n",
        "  })\n",
        "\n",
        "  tryCatch(expr = {\n",
        "    fit.EXP <- gamlssMXfits(n=1,var~1, family=EXP, K=k, data = NULL)\n",
        "    families <- append(families, \"EXP\")\n",
        "    AICs <- append(AICs, -AIC(fit.EXP))\n",
        "    logLiks <- append(logLiks, logLik(fit.EXP))\n",
        "  }, error = function(e){\n",
        "    print(\"Error in EXP, passing over...\")\n",
        "  })\n",
        "\n",
        "  tryCatch(expr = {\n",
        "    fit.IGAMMA <- gamlssMXfits(n=1,var~1, family=IGAMMA, K=k, data = NULL)\n",
        "    families <- append(families, \"IGAMMA\")\n",
        "    AICs <- append(AICs, -AIC(fit.IGAMMA))\n",
        "    logLiks <- append(logLiks, logLik(fit.IGAMMA))\n",
        "  }, error = function(e){\n",
        "    print(\"Error in IGAMMA, passing over...\")\n",
        "  })\n",
        "\n",
        "  tryCatch(expr = {\n",
        "    fit.IG <- gamlssMXfits(n=1,var~1, family=IG, K=k, data = NULL)\n",
        "    families <- append(families, \"IG\")\n",
        "    AICs <- append(AICs, -AIC(fit.IG))\n",
        "    logLiks <- append(logLiks, logLik(fit.IG))\n",
        "  }, error = function(e){\n",
        "    print(\"Error in IG, passing over...\")\n",
        "  })\n",
        "\n",
        "  tryCatch(expr = {\n",
        "    fit.JSU <- gamlssMXfits(n=1,var~1, family=JSU, K=k, data = NULL)\n",
        "    families <- append(families, \"JSU\")\n",
        "    AICs <- append(AICs, -AIC(fit.JSU))\n",
        "    logLiks <- append(logLiks, logLik(fit.JSU))\n",
        "  }, error = function(e){\n",
        "    print(\"Error in JSU, passing over...\")\n",
        "  })\n",
        "\n",
        "  tryCatch(expr = {\n",
        "    fit.GG <- gamlssMXfits(n=1,var~1, family=GG, K=k, data = NULL)\n",
        "    families <- append(families, \"GG\")\n",
        "    AICs <- append(AICs, -AIC(fit.GG))\n",
        "    logLiks <- append(logLiks, logLik(fit.GG))\n",
        "  }, error = function(e){\n",
        "    print(\"Error in GG, passing over...\")\n",
        "  })\n",
        "\n",
        "  tryCatch(expr = {\n",
        "    fit.LO <- gamlssMXfits(n=1,var~1, family=LO, K=k, data = NULL)\n",
        "    families <- append(families, \"LO\")\n",
        "    AICs <- append(AICs, -AIC(fit.LO))\n",
        "    logLiks <- append(logLiks, logLik(fit.LO))\n",
        "  }, error = function(e){\n",
        "    print(\"Error in LO, passing over...\")\n",
        "  })\n",
        "\n",
        "  tryCatch(expr = {\n",
        "    fit.LOGNO <- gamlssMXfits(n=1,var~1, family=LOGNO, K=k, data = NULL)\n",
        "    families <- append(families, \"LOGNO\")\n",
        "    AICs <- append(AICs, -AIC(fit.LOGNO))\n",
        "    logLiks <- append(logLiks, logLik(fit.LOGNO))\n",
        "  }, error = function(e){\n",
        "    print(\"Error in LOGNO, passing over...\")\n",
        "  })\n",
        "\n",
        "  tryCatch(expr = {\n",
        "    fit.WEI <- gamlssMXfits(n=1,var~1, family=WEI, K=k, data = NULL)\n",
        "    families <- append(families, \"WEI\")\n",
        "    AICs <- append(AICs, -AIC(fit.WEI))\n",
        "    logLiks <- append(logLiks, logLik(fit.WEI))\n",
        "  }, error = function(e){\n",
        "    print(\"Error in WEI, passing over...\")\n",
        "  })\n",
        "  \n",
        "\n",
        "  results <- data.frame(family = families, AIC = AICs, logLik = logLiks)\n",
        "  showBest(results)\n",
        "  return(results)\n",
        "\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fZMqFXQIwsoO"
      },
      "source": [
        "#n1 <- 200\n",
        "#n2 <- 300\n",
        "#n  <- n1 + n2\n",
        "#X1 <- rJSU(n1, 0.001, 0.1, 0.14, 0.7)\n",
        "#X2 <- rJSU(n2, 0.8, 0.04, 1, 3)\n",
        "#XX <- c(X1,X2)\n",
        "\n",
        "#fit <- gamlssMXfits(n = 1, XX~1, family = JSU, K = 2, data = NULL)\n",
        "#mu <- get.mu.hats(fit, 2)\n",
        "#sigma <- get.sigma.hats(fit, 2)\n",
        "#nu <- get.nu.hats(fit, 2)\n",
        "#tau <- get.tau.hats(fit,2)\n",
        "#plot.gamlss(var = XX, family = \"JSU\", fit = fit, K = 2, mu.hats = mu, sigma.hats = sigma, nu.hats = nu, tau.hats = tau)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j2DSlgQgj8gz"
      },
      "source": [
        "First of all we will get a random sampled version of the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sQdbSfiGj8gz"
      },
      "source": [
        "set.seed(12999)\n",
        "\n",
        "rows <- sample(1:nrow(insurance.data), 5000)\n",
        "\n",
        "insurance.data <- insurance.data[rows,]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0gfOxdc0j8gz"
      },
      "source": [
        "Let's show the table."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GxTG5Te4j8gz"
      },
      "source": [
        "head(insurance.data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l_f_zHnpj8gz"
      },
      "source": [
        "Now let's see if there are missing values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NFDrrjvDj8gz"
      },
      "source": [
        "# for each value check if is missing or not\n",
        "df.na <- is.na(insurance.data)\n",
        "\n",
        "# for each column check how many missing values are there\n",
        "apply(df.na, 2, sum)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_SurVOxvj8gz"
      },
      "source": [
        "Fortunately the dataset has not any missing values, so we can go on with our analysis without any fear.\n",
        "\n",
        "Initially we will analyze and discuss the dataset structure."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JLeDcaQHj8gz"
      },
      "source": [
        "str(insurance.data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iNQEjJvlj8gz"
      },
      "source": [
        "This dataset contains $n = 5000$ observations and $d = 12$ columns. Let's understand something about the variables:\n",
        "\n",
        "### 1.1 ID"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MKVq41vdj8gz"
      },
      "source": [
        "str(insurance.data$id)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T6lCrjUfj8gz"
      },
      "source": [
        "It contains the identification number of each row and seems to be a 'row number count' variable with no particular informations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KbicXPXJj8gz"
      },
      "source": [
        "# loading the rownames as integer into the 'indexs' variable\n",
        "indexs <- as.integer(rownames(insurance.data))\n",
        "\n",
        "# loading the id column into the 'ids' variable\n",
        "ids <- insurance.data$id\n",
        "\n",
        "# checking if the two vectors are equal\n",
        "min(indexs == ids) == TRUE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RprbmmIIj8gz"
      },
      "source": [
        "Since the 'id' column is exactly a 'row number count' variable whe can delete it without losing informations "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0JxwpBSfj8gz"
      },
      "source": [
        "insurance.data$id <- NULL"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ThFbPQcqj8gz"
      },
      "source": [
        "according to the reducted dimension of the dataset we will set the rownames as the vector  $[1,...,nrow(dataset)]$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ASzatQlFj8gz"
      },
      "source": [
        "rownames(insurance.data) <- 1:nrow(insurance.data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5u8dvZIQj8g0"
      },
      "source": [
        "### 1.2. Gender"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z4J2POtEj8g0"
      },
      "source": [
        "str(insurance.data$Gender)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tn7AI9Iuj8g0"
      },
      "source": [
        "This is a categorical variable, containing informations about the gender of a potential customer.\n",
        "\n",
        "We will transform it in a factor variable with 2 levels: \"Female\" and \"Male\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ObtKTAOKycXb"
      },
      "source": [
        "insurance.data$Gender <- as.factor(insurance.data$Gender)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Okiv1yJej8g0"
      },
      "source": [
        "print(head(insurance.data$Gender))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B2jEUNIij8g0"
      },
      "source": [
        "Let's see how the two genders are distributed into the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bfpQ3OPlj8g0"
      },
      "source": [
        "insurance.data %>%\n",
        "    ggplot(aes(x = Gender, fill = Gender)) +\n",
        "    geom_bar(aes(y = (..count..)/sum(..count..))) +\n",
        "    ylab('frequency')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JG_cEHLEj8g0"
      },
      "source": [
        "As we can see thanks to the bar plot the two genders are pretty equally distributed into the dataset, since the **Females** are approximately the $45\\%$ of the data and the **Males** are approximately the $55\\%$ of the data.\n",
        "\n",
        "### 1.3. Age"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ryG2_gcDj8g0"
      },
      "source": [
        "str(insurance.data$Age)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c1tp-qspj8g0"
      },
      "source": [
        "This is an integer variable containing the potential customer's age.\n",
        "This could be an important information since the higher the age is the more the risks are and the insurance premium could become higher, but the thing is also true in reverse, as younger people are less experienced and generally more reckless.\n",
        "\n",
        "Let's see how this important variable is distributed"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x0d9BQZqj8g0"
      },
      "source": [
        "insurance.data %>%\n",
        "    ggplot(aes(x = Age)) +\n",
        "    geom_bar(aes(y = (..count..)/sum(..count..)), fill = \"blue\") +\n",
        "    ylab(\"frequency\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z4cyne1Rj8g0"
      },
      "source": [
        "It's interesting to see that the main part of the potential customers is between $20$ and $30$ years-old, it could mean that they are looking for their first insurance contract, so, since they are freshman they could be convinced easilier to sign with the company.\n",
        "\n",
        "We can also see another peak in the plot, at the age interval $40-50$. This may be due to people having earned a fair amount of money and being looking for a better insurance contract.\n",
        "\n",
        "Now we will try to fit some models to our variable.\n",
        "First of all we will analyze and plot three of these models then we will test an higher number of families and pick up the best one.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S_pxPNQ_83G2"
      },
      "source": [
        "#c(GA, BE, EXP, IGAMMA, IG, JSU, GG, GU, LO, LOGNO, WEI)\n",
        "fit.GA <- gamlssMXfits(n=5,Age~1,data = insurance.data, family=GA, K=2)\n",
        "\n",
        "mu <- get.mu.hats(fit.GA, 2)\n",
        "sigma <- get.sigma.hats(fit.GA, 2)\n",
        "\n",
        "plot.gamlss(insurance.data$Age, \"GA\", fit.GA, 2, mu, sigma)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F1VulqShiCyl"
      },
      "source": [
        "Using two gamma models seems to be a very nice choice in fitting our data, the sum of the two distributions follow the original variable in a pretty nice way."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UcnTdjq3fDkU"
      },
      "source": [
        "fit.EXP <- gamlssMXfits(n=5,Age~1,data = insurance.data, family=EXP, K=2)\n",
        "\n",
        "mu <- get.mu.hats(fit.EXP, 2)\n",
        "\n",
        "plot.gamlss(insurance.data$Age, \"EXP\", fit.EXP, 2, mu)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uq1J21rejSF2"
      },
      "source": [
        "As we can see using two exponential distribution to fit our data was a really bad idea, but we could have expected so since the behaviour of the exponential distribution is very different from ours.\n",
        "\n",
        "Now we will plot the last distribution family befor applying an automated function to compare $11$ different distribution families and taking the best one according to the majority rule testing **AIC** and **LogLikelihood** "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "omXuslN5g4B0"
      },
      "source": [
        "fit.GU <- gamlssMXfits(n=5,Age~1,data = insurance.data, family=GU, K=2)\n",
        "\n",
        "mu <- get.mu.hats(fit.GU, 2)\n",
        "sigma <- get.sigma.hats(fit.GU, 2)\n",
        "\n",
        "plot.gamlss(var = insurance.data$Age, family = \"GU\", fit = fit.GU, K = 2, mu.hats = mu, sigma)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NJq24WOtk418"
      },
      "source": [
        "Also the Gumbel distribution did not worked very well, but it was better than the exponential one.\n",
        "\n",
        "Now we will analyze all the families in one shot thanks to a function we create above which allows us to compare all the families and taking the best one!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KrC46y9l-CTT"
      },
      "source": [
        "tests <- test.all.gamlss.mx(insurance.data$Age, k = 2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jhpxS685gk-1"
      },
      "source": [
        "fit.GG <- gamlssMXfits(n=5,Age~1,data = insurance.data, family=GG, K=2)\n",
        "\n",
        "mu <- get.mu.hats(fit.GG, 2)\n",
        "sigma <- get.sigma.hats(fit.GG, 2)\n",
        "nu <- get.nu.hats(fit.GG, 2)\n",
        "\n",
        "plot.gamlss(var = insurance.data$Age, family = \"GG\", fit = fit.GG, K = 2, mu.hats = mu, sigma, nu)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ExYtas8832w"
      },
      "source": [
        "### 1.4. Driving_License"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z7cd-U7Sj8g0"
      },
      "source": [
        "str(insurance.data$Driving_License)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6QZEADd6j8g0"
      },
      "source": [
        "This is an integer variable and it may be ambiguous since it could count how many driving licenses one person have (car, truck, motorbike, ...) or it could just be a boolean value indicating if one person has a driving lincense or not.\n",
        "\n",
        "First of all we will see how the data are distributed"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SxaesfzIj8g0"
      },
      "source": [
        "insurance.data %>%\n",
        "    ggplot(aes(x = Driving_License)) +\n",
        "    geom_bar(aes(y = (..count..)/sum(..count..)),, fill = \"blue\") +\n",
        "    ylab(\"frequency\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SXCRnr3Mj8g0"
      },
      "source": [
        "As we can see the variable act like a boolean one. This variable seems to be pretty useless since approximately the $100\\%$ of the people has a driving license.\n",
        "\n",
        "But, the absence of the driving license may significantly change the premium amount and be a crucial factor in the response of the non-driving people.\n",
        "\n",
        "In order to make the dataset consistent we will transform this variable in a categorical one"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oj21gEuGj8g0"
      },
      "source": [
        "insurance.data$Driving_License <- as.factor(if_else(insurance.data$Driving_License == 1, \"Yes\", \"No\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JjbKN9Ihj8g0"
      },
      "source": [
        "### 1.5. Region_Code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VHW1Yvw4j8g0"
      },
      "source": [
        "str(insurance.data$Region_Code)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JCBma6T4j8g0"
      },
      "source": [
        "This integer variable contains an unique code for the region of the customer. Let's see the distribution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n6D1Ht8Oj8g0"
      },
      "source": [
        "insurance.data %>%\n",
        "    ggplot(aes(x = Region_Code)) +\n",
        "    geom_bar(aes(y = (..count..)/sum(..count..)),fill = \"blue\") +\n",
        "    ylab(\"frequency\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TQqlAXF7j8g1"
      },
      "source": [
        "from this graph we can see that most of the customers come from $7$ of the $52$ regions.\n",
        "\n",
        "Since this seems to be a multimodal variable we will fit all the families as we did before."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gs_ZBNgl_cDh"
      },
      "source": [
        "tests <- test.all.gamlss.mx(insurance.data$Region_Code, k = 2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E88zWuArNmEd"
      },
      "source": [
        "Our function gave us that the *LO* model has best result on both the metric!\n",
        "Let's plot it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qeEftGZ6N8v0"
      },
      "source": [
        "fit <- gamlssMXfits(n = 1, Region_Code~1, family = LO, K = 2, data = insurance.data)\n",
        "\n",
        "mu <- get.mu.hats(fit, 2)\n",
        "sigma <- get.sigma.hats(fit, 2)\n",
        "\n",
        "plot.gamlss(var = insurance.data$Region_Code, family = \"LO\", fit = fit, K = 2, mu.hats = mu, sigma.hats = sigma)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9rjJMI7wWa7I"
      },
      "source": [
        "Even the best model fits in a bad way our variable, but more or less it caught the main behaviour of the data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PSKOjOwo_ITP"
      },
      "source": [
        "### 1.6. Previously_Insured"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_qHty6TZj8g1"
      },
      "source": [
        "str(insurance.data$Previously_Insured)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WIf5vkBRj8g1"
      },
      "source": [
        "This is another integer logical like variable that contains information about any previous vehicle insurance. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T0p685llj8g1"
      },
      "source": [
        "insurance.data %>%\n",
        "    ggplot(aes(x = Previously_Insured, fill = Previously_Insured)) +\n",
        "    geom_bar(aes(y = (..count..)/sum(..count..)), fill = \"blue\") +\n",
        "    ylab(\"frequency\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jrBSR4h_j8g1"
      },
      "source": [
        "Also this variable seems to be pretty equally distributed with a small overcoming of people with have no any insurance yet, has to be noted that this could be related to the fact that the most part of the dataset is composed by young people with their (probably) first car!\n",
        "\n",
        "Let's make this variable categorical"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vf1BnAIpj8g1"
      },
      "source": [
        "insurance.data$Previously_Insured <- as.factor(if_else(insurance.data$Previously_Insured == 1, \"Yes\", \"No\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JDyGo866j8g1"
      },
      "source": [
        "### 1.7. Vehicle_Age"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g4OPpjVBj8g1"
      },
      "source": [
        "str(insurance.data$Vehicle_Age)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I_JaRfIWj8g1"
      },
      "source": [
        "This variable is a 3 levels factor, since the car's age is an important factor due to the amount of the annual premium, it could make sense to think that the older it is the higher the prize will be, we need to make sure these factors are ordered"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HANlvo0tj8g1"
      },
      "source": [
        "is.ordered(insurance.data$Vehicle_Age)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VCA3FspCj8g1"
      },
      "source": [
        "The variable is not ordered, so we have to do this by our own"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TRDXZvDFj8g1"
      },
      "source": [
        "levels(insurance.data$Vehicle_Age)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3AVI2w7Zj8g1"
      },
      "source": [
        "# ordering the factors\n",
        "insurance.data$Vehicle_Age <- ordered(insurance.data$Vehicle_Age, levels = c('< 1 Year','1-2 Year', '> 2 Years'))\n",
        "\n",
        "# check if the variable becomes ordered\n",
        "is.ordered(insurance.data$Vehicle_Age)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H9sFKwASj8g1"
      },
      "source": [
        "# check that the order is correct\n",
        "print(head(insurance.data$Vehicle_Age))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xoFXXqPNj8g1"
      },
      "source": [
        "Now we can see how the factors are distributed"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dY6zHLO7j8g1"
      },
      "source": [
        "insurance.data %>%\n",
        "    ggplot(aes(x = Vehicle_Age, fill = Vehicle_Age)) +\n",
        "    geom_bar(aes(y = (..count..)/sum(..count..))) +\n",
        "    ylab(\"frequency\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CPp6nbCYj8g1"
      },
      "source": [
        "The majority of the cars are pretty new, so may have sense to think that this variable has not a lot of utility.\n",
        "\n",
        "Since it is an ordered categorical variable we preffer to transform it in a numeric variable"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AarGfUrej8g1"
      },
      "source": [
        "insurance.data$Vehicle_Age <- as.integer(insurance.data$Vehicle_Age)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "acfa0suaj8g1"
      },
      "source": [
        "### 1.8. Vehicle_Damage\t"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "03JyxFoej8g1"
      },
      "source": [
        "str(insurance.data$Vehicle_Damage)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rH7rKNMSj8g2"
      },
      "source": [
        "This is a char variable acting like a 2-level factorial variable, containing information on whether or not the customer got his/her vehicle damaged. \n",
        "\n",
        "We are going to transform it in a factor variable."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VKETrvWyy336"
      },
      "source": [
        "insurance.data$Vehicle_Damage <- as.factor(insurance.data$Vehicle_Damage)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FYWJrQ05j8g2"
      },
      "source": [
        "\n",
        "Let's see the distribution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y6v1qpNhj8g2"
      },
      "source": [
        "insurance.data %>%\n",
        "    ggplot(aes(x = Vehicle_Damage, fill = Vehicle_Damage)) +\n",
        "    geom_bar(aes(y = (..count..)/sum(..count..))) +\n",
        "    ylab(\"frequency\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qwsIYLmNj8g2"
      },
      "source": [
        "The variable is \n",
        "almost perfectly balanced, so no comments can be made\n",
        "\n",
        "### 1.9. Annual_Premium"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vuRgXoKQj8g2"
      },
      "source": [
        "str(insurance.data$Annual_Premium)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "giZLkZcIj8g2"
      },
      "source": [
        "Numerical variable indicating the amount customer needs to pay as premium in the year"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "22k7CSkAj8g2"
      },
      "source": [
        "# get some break points for a better visualization of the x-axis\n",
        "breaks.x <- c(\n",
        "    min(insurance.data$Annual_Premium),\n",
        "    max(insurance.data$Annual_Premium)/4,\n",
        "    max(insurance.data$Annual_Premium)/2,\n",
        "    max(insurance.data$Annual_Premium)/1.35,\n",
        "    max(insurance.data$Annual_Premium))\n",
        "    \n",
        "insurance.data %>%\n",
        "    ggplot(aes(x = Annual_Premium)) +\n",
        "    geom_histogram(aes(y = ..density..),bins = 45, fill = \"blue\") +\n",
        "    scale_x_continuous(breaks = breaks.x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OrHjiqsIj8g2"
      },
      "source": [
        "We can see that the most premium is in the interval $(2630,115000)$ with a really low number of people which falls out of that interval.\n",
        "\n",
        "This variable seems to be unimodal, so we will fit a single distribution on this variable."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WxPWgYXgAJB1"
      },
      "source": [
        "EXP.fit <- histDist(insurance.data$Annual_Premium, family = EXP, nbins = 70)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R7UWpJKJa52Y"
      },
      "source": [
        "PE.fit <- histDist(insurance.data$Annual_Premium, family = PE, nbins = 40)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zv4O2LezbxOQ"
      },
      "source": [
        "WEI.fit <- histDist(insurance.data$Annual_Premium, family = NOF, nbins = 40)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DKCwiQaJcCBZ"
      },
      "source": [
        "The single distributions seems to not work properly, so we will fit a bivariate model as usual"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nHjeKOv7k6pN"
      },
      "source": [
        "tests <- test.all.gamlss.mx(insurance.data$Annual_Premium, k = 2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AfZm48NLcBaF"
      },
      "source": [
        "fit.GA <- gamlssMXfits(n = 1, Annual_Premium~1, family = GA, K = 2, data = insurance.data)\n",
        "\n",
        "\n",
        "mu <- get.mu.hats(fit.GA, 2)\n",
        "sigma <- get.sigma.hats(fit.GA, 2)\n",
        "\n",
        "plot.gamlss(var = insurance.data$Annual_Premium, family = \"GA\", fit = fit.GA, K = 2, mu.hats = mu, sigma.hats = sigma)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k022F9VUo65G"
      },
      "source": [
        "As we can see using two models helped a lot but we are still far away from a perfect fit!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MdaFbwYqj8g2"
      },
      "source": [
        "### 1.10. Policy_Sales_Channel"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "24oyCbdtj8g2"
      },
      "source": [
        "str(insurance.data$Policy_Sales_Channel)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fww3h7Dcj8g2"
      },
      "source": [
        "This variable contains anonymized Code for the channel of outreaching to the customer ie. Different Agents, Over Mail, Over Phone, In Person, etc."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R33tUF7cj8g2"
      },
      "source": [
        "insurance.data %>%\n",
        "    ggplot(aes(x = Policy_Sales_Channel)) +\n",
        "    geom_bar(aes(y = (..count..)/sum(..count..)),fill = \"blue\") +\n",
        "    ylab(\"frequency\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JbqDyqu7j8g2"
      },
      "source": [
        "There are few main policy sales channels, but due to the fact that there are no informations about what a channel refers to, we can't do any discussion."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mddF9Ufrj8g2"
      },
      "source": [
        "### 1.11. Vintage"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sY3UrCWKj8g2"
      },
      "source": [
        "str(insurance.data$Vintage)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YicIvlbRj8g2"
      },
      "source": [
        "This variable contains the number of days a customer has been associated with the company. The fidelity of a person could be a nice bias in order to sign the contract. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "llIG1IwOj8g2"
      },
      "source": [
        "Let's see the distribution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WP3PFVydj8g2"
      },
      "source": [
        "insurance.data %>%\n",
        "    ggplot(aes(x = Vintage)) +\n",
        "    geom_bar(aes(y = ..count..),fill = \"red\") +\n",
        "    geom_density(aes(y = ..count..),color = \"blue\") +\n",
        "    ylab(\"frequency\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qZBDPPpsj8g2"
      },
      "source": [
        "The variable seems to be uniformly distributed.\n",
        "\n",
        "Let's test this whit a significance level $\\alpha = 0.05$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QEAnRMWCj8g2"
      },
      "source": [
        "scaled.vintage <- scale(insurance.data$Vintage, scale = T, center = F)\n",
        "min <- min(scaled.vintage)\n",
        "max <- max(scaled.vintage)\n",
        "test <- spgs::chisq.unif.test(scaled.vintage, interval = c(min,max), bins = nrow(insurance.data))\n",
        "test$p.value"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WUBDZzupj8g2"
      },
      "source": [
        "Since whe have that $p-value > \\alpha = 0.05$ the test told us that we cannot reject the hypotesis that the variable is uniformly distributed!\n",
        "\n",
        "We have that the dataset is equally distributed between fresh customers and older ones. But also the older ones are customers for less than one year!\n",
        "\n",
        "### 1.12. Response\n",
        "\n",
        "This is the label column, since we want do some unsupervised analysis we will take it away from the dataset and saving in a separate variable. We will look at this variable only to do external clustering validation causo looking at it during our analysis could be an high bias on our analysis and choices."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XBQZ01Cej8g2"
      },
      "source": [
        "# saving the response in a separate variable\n",
        "response.var <- insurance.data$Response\n",
        "\n",
        "# delete the column from the dataset\n",
        "insurance.data$Response <- NULL"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PBbQlKdbj8g2"
      },
      "source": [
        "<hr style=\"height:2px;border-width:0;color:black;background-color:black\">\n",
        "\n",
        "## <center> Variables Recap</center>\n",
        "\n",
        "|#| Name | Type | Description |\n",
        "|:-| :- | -: | :-: |\n",
        "|1| [Gender](#1.2.-Gender) | Factor | Gender of the customer|\n",
        "|2| [Age](#1.3.-Age) | Integer | Age of the customer |\n",
        "|3| [Driving_License](#1.4.-Driving_License) | Factor | &emsp;&emsp;&emsp; <span style=\"color:green\">**Yes**</span>: Customer already has DL; <span style=\"color:red\">**No**</span>: Customer does not have DL| \n",
        "|4| [Region_Code](#1.5.-Region_Code) | Integer | Unique code for the region of the customer |\n",
        "|5| [Previously_Insured](#1.6.-Previously_Insured) | Factor | &emsp;&emsp;&emsp; <span style=\"color:green\">**Yes**</span>: Customer already has Vehicle Insurance;<span style=\"color:red\">**No**</span>: Customer doesn't have Vehicle Insurance|\n",
        "|6|[Vehicle_Age](#1.7.-Vehicle_Age)|Integer| Age of the Vehicle|\n",
        "|7|[Vehicle_Damage](#1.8.-Vehicle_Damage)|Factor|&emsp;&emsp;&emsp; <span style=\"color:green\">**Yes**</span>:Customer got his/her vehicle damaged in the past;<span style=\"color:red\">**No**</span>:Customer didn't get his/her vehicle damaged in the past|\n",
        "|8|[Annual_Premium](#1.9.-Annual_Premium)|Numeric|The amount customer needs to pay as premium in the year|\n",
        "|9|[Policy_Sales_Channel](#1.10.-Policy_Sales_Channel)|Integer|Anonymized Code for the channel of outreaching to the customer ie. Different Agents, Over Mail, Over Phone, In Person, etc.|\n",
        "|10| [Vintage](#1.11.-Vintage)|Integer|Number of Days the Customer has been associated with the company|\n",
        "\n",
        "<hr style=\"height:2px;border-width:0;color:black;background-color:black\">\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iyn2CyEIj8g3"
      },
      "source": [
        "## 2. Preparing the data\n",
        "\n",
        "Before going on with our analysis we need to apply some transformations to the dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iGzK1YrFj8g3"
      },
      "source": [
        "\n",
        "### 2.1 Data Normalization\n",
        "\n",
        "We have really different data ranges in our dataset, so we have to scale the data in order to make the future analysis indipendent from the scale of the data.\n",
        "\n",
        "**NOTE**: this does not affect the data distributions seen above and their respective discussions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RTgxu23qj8g3"
      },
      "source": [
        "# get the numeric variables\n",
        "num_cols <- unlist(lapply(insurance.data, is.numeric))  \n",
        "\n",
        "# scale the numeric variables\n",
        "insurance.scaled <- insurance.data\n",
        "insurance.scaled[,num_cols] <- as.data.frame(scale(insurance.data[num_cols],center = F, scale = T))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L8XVode-j8g3"
      },
      "source": [
        "Let's show the new dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "udmKd5Ybj8g3"
      },
      "source": [
        "head(insurance.scaled)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WTT_rYnRj8g3"
      },
      "source": [
        "## 3. Correlation analysis\n",
        "\n",
        "It's important to see if there is any kind of relations between variables.\n",
        "\n",
        "### 3.1 Pearson's correlation\n",
        "\n",
        "Let's start checking if there is correlation between numerical values.\n",
        "\n",
        "First of all we will define a function to create a matrix of p-value according to the Pearons's linearity test, after we will calculate the Pearson's correlation matrix and the p-values matrix to check if there is a linear depencies between the variables. We will use a significativity level of $0.01$ to test our linearity correlation hipothesis. At the end we will show the results via an heatmap. In the next figure, correlations with p-value > 0.01 are considered as insignificant. In this case the correlation coefficient values are leaved blank and crosses are added."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KzjGtDg-j8g3"
      },
      "source": [
        "# a function to create a matrix filled by p-value of the correlation tests\n",
        "cor.mtest <- function(mat, ...) {\n",
        "    mat <- as.matrix(mat)\n",
        "    n <- ncol(mat)\n",
        "    p.mat <- matrix(NA, n, n)\n",
        "    for (i in 1:n) {\n",
        "        for (j in 1:n) {\n",
        "            tmp <- cor.test(mat[, i], mat[, j], ...)\n",
        "            p.mat[i, j] <- tmp$p.value\n",
        "        }\n",
        "    }\n",
        "  colnames(p.mat) <- rownames(p.mat) <- colnames(mat)\n",
        "  return(p.mat)\n",
        "}\n",
        "\n",
        "# calculate the correlation matrix\n",
        "cor <- cor(insurance.scaled[,num_cols], method = \"pearson\")\n",
        "\n",
        "# calculate the correlation p-value matrix\n",
        "p.mat.pears <- cor.mtest(insurance.scaled[,num_cols])\n",
        "\n",
        "# showing the correlation matrix with an heatmap\n",
        "corrplot(cor, type = \"lower\", p.mat = p.mat.pears, sig.level = 0.01, method = \"color\",addCoef.col = \"black\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KpY51Snwj8g3"
      },
      "source": [
        "One thing that we can see in the plot is that the vehicle age is positively correlated to the customers' age.\n",
        "\n",
        "There is also a negative correlation between **policy_sales_channel** and **Age**, but since this is a non ordered categorical variable it makes no sense to consider that information, however if we had known that the policy_sales_channel has been chronologically ordered (the $i^{th}$ sales channel was added after the $(i-1)^{th}$ sales channel and before the $(i+1)^{th}$ sales channel) we had that newer (bigger value) sales channels work better with younger customers and older (lower value) sales channels work better with older customers!\n",
        "\n",
        "Overall we can see that there are only few variables that are strongly linearly correlated.\n",
        "\n",
        "Note that the Pearson correlation matrix shows only if two variables are **linearly** correlated, so makes sense study also other relationships based on our intuition in order to see if there are any non-linear correlation between data!\n",
        "\n",
        "### 3.2 Cramer's V for correlation\n",
        "\n",
        "Now let's apply the Cramer's correlation to check if there is any form of correlation between categorical variables. It is a correlation measure based on $\\chi^2$ chi-squared method via the following formula: $$V = \\sqrt{\\frac{\\chi^2}{(k-1)n}}$$ where:\n",
        "  * $\\chi^2$ is chi-squared\n",
        "  * $n$ is the number of observation\n",
        "  * $k$ is the number of columns\n",
        "\n",
        "It takes values in $[0,1]$ such that a value close to $1$ sayis to us there is a nice correlation between the two variables."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LiKUPGkGj8g3"
      },
      "source": [
        "# a function to create a matrix filled by cramers value for the correlation\n",
        "cram.cor <- function(mat, ...) {\n",
        "    mat <- as.matrix(mat)\n",
        "    n <- ncol(mat)\n",
        "    p.mat <- matrix(NA, n, n)\n",
        "    for (i in 1:n) {\n",
        "        for (j in 1:n) {\n",
        "            tmp <- assocstats(table(mat[,i],mat[,j]))\n",
        "            p.mat[i, j] <- tmp$cramer\n",
        "        }\n",
        "    }\n",
        "  colnames(p.mat) <- rownames(p.mat) <- colnames(mat)\n",
        "  return(p.mat)\n",
        "}\n",
        "\n",
        "cram.matrix <- cram.cor(insurance.scaled[,!num_cols])\n",
        "\n",
        "corrplot(cram.matrix, type = \"lower\", method = \"color\", is.corr = F, addCoef.col = \"black\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OSGAaMTjj8g3"
      },
      "source": [
        "Here we have plotted the cramers' values.\n",
        "\n",
        "We can see that *gender* is weakly related with *Previously_Insured* and *Vehicle_Damage*, instead *Vehicle_Damage* is highly related to *Previously_Insured*!\n",
        "\n",
        "### 3.3 Biserial correlation\n",
        "\n",
        "Finally we will analyze if there is any form of correlation between mixed (numeric vs categorical) variables, we are going to to this by applying the Biserial correlation.\n",
        "\n",
        "The point Biserial correlation is a correlation coefficient used when one variable is dichotomus and the other one is continuous. It is mathematically equivalent to the Pearson correlation.\n",
        "\n",
        "The Biserial correlation is calculated by splitting the data set into two groups, group 1 which received the value $1$ on the dichotomus variable and group $2$ which received the value $0$, then the point-biserial correlation coefficient is calculated as follows: $$r_{pb}=\\frac{M_1 - M_0}{s_n}\\sqrt{\\frac{n_1 n_2}{n^2}}$$ where $$s_n = \\sqrt{\\frac{1}{n}\\sum_{i=1}^n (X_i - \\bar{X})^2}$$ is the standard deviation and where:\n",
        "  * $X$ is the continuous variable\n",
        "  * $M_1$ is the mean value of the continuous variable for all data in group $1$\n",
        "  * $M_0$ is the mean value of the continuous variable for all data in group $0$\n",
        "  * $n_1$ is the number of data points in group $1$\n",
        "  * $n_0$ is the number of data points in group $0$\n",
        "  * $n = n_1 + n_2$\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9bq6PzGtj8g3"
      },
      "source": [
        "biserial.cormat <- function(mat1, mat2, ...) {\n",
        "    n1 <- ncol(mat1)\n",
        "    n2 <- ncol(mat2)\n",
        "    p.mat <- matrix(NA, n1, n2)\n",
        "    for (i in 1:n1) {\n",
        "        for (j in 1:n2) {\n",
        "            if(is.numeric(mat1[,i]) == T && is.factor(mat2[,j]) == T){\n",
        "                tmp <- biserial.cor(x = mat1[,i], y = mat2[,j], use = \"all.obs\")\n",
        "                p.mat[i, j] <- tmp\n",
        "            }\n",
        "\n",
        "        }\n",
        "    }\n",
        "  colnames(p.mat) <- colnames(mat2)\n",
        "  rownames(p.mat) <- colnames(mat1)\n",
        "  return(t(p.mat))\n",
        "}\n",
        "\n",
        "biserial.matrix <- biserial.cormat(insurance.scaled[,num_cols], insurance.scaled[,!num_cols])\n",
        "corrplot(biserial.matrix,method = \"color\", is.corr = F, addCoef.col = \"black\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_a6sneBtj8g3"
      },
      "source": [
        "Thanks to the Biserial correlation we can see that there is correlation between numerical and categorical variables, except *Region_Code*,*Annual_Premium* and *Vintage* that seem to are indipendent from all the categorical variables."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lx04zxOCj8g3"
      },
      "source": [
        "## 4. Multivariate analysis\n",
        "\n",
        "We will analyze in depth the relationships between some interesting variables in order to have a better understanding about the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KklusLDBj8g3"
      },
      "source": [
        "### 4.1. Age~Vehicle_Age\n",
        "\n",
        "Let's see a box plot that reppresent these two variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "03K35ljij8g3"
      },
      "source": [
        "insurance.data %>%\n",
        "    ggplot(aes(x = Vehicle_Age, y = Age)) +\n",
        "    geom_boxplot(aes(group=Vehicle_Age, colour = as.factor(Vehicle_Age))) +\n",
        "    guides(color=guide_legend(title=\"Vehicle_Age\")) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OABd6I9Fj8g3"
      },
      "source": [
        "Looking at the plot we can see that the newest cars ($< 1$ year) are driven by people between $20$ and $30$ years, the mid-age cars (between $1$ and $2$ years) are driven by mid-age people ($40-60$ years old) and the older cars are driven also by the mid-age people but few years older. According to the Vehicle Age [plot](#1.7.-Vehicle_Age) we know there are a really low number of old cars, so we have that the customers, in general, all have discrete security systems in their cars!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1oJ0P4eOj8g3"
      },
      "source": [
        "### 4.2. Age~Vehicle_Damage\n",
        "\n",
        "It's important to see if people have their vehicles damaged with a certain relation to their ages."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6F8jtlrTj8g3"
      },
      "source": [
        "insurance.data %>%\n",
        "    ggplot(aes(x = Age)) +\n",
        "    geom_histogram(aes(y = ..count../sum(..count..),fill = Vehicle_Damage), bins = 40)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t582jOxJj8g3"
      },
      "source": [
        "Thanks to the plot we know that in general people tend to not have their vehicle damaged. We have also that youger people are more cautious than older ones. We can also see that the differences between red (no damages) and blue (damaged cars) bars become lower and lower at the increasing of the age."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lEo59hpbj8g3"
      },
      "source": [
        "### 4.3. Vehicle_Age~Vehicle_Damage\n",
        "\n",
        "We already know that there is a certain form of relation between these data, this could be due the more a car is driven the more are the probability to get damaged."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QW5rDnloj8g3"
      },
      "source": [
        "insurance.data %>%\n",
        "    ggplot(aes(x = as.factor(Vehicle_Age))) +\n",
        "    geom_bar(aes(y =..count../sum(..count..), fill = Vehicle_Damage), position = \"dodge\") +\n",
        "    xlab(\"Vehicle_Age\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Szcpnin9j8g3"
      },
      "source": [
        "The plot seems to accord our expectations, since we can see that the proportion of damaged cars increase at the increasing of the age of the cars"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RMnC0uqoj8g3"
      },
      "source": [
        "### 4.4. Age~Annual_Premium\n",
        "\n",
        "According to the two discussions above, it is reasonable to think that older people, having on average older cars, are more at risk of accidents than younger people. Hence, let's see if the age is a relevant factor on the calculation of the annual premium"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-fXUn8i9j8g3"
      },
      "source": [
        "insurance.data %>%\n",
        "    ggplot(aes(x = Age, y = Annual_Premium)) + \n",
        "    geom_point() +\n",
        "    geom_smooth(formula =  y ~ x, method = \"loess\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CUuvMRGZj8g3"
      },
      "source": [
        "No relations seems to be between these two features, since the annual premium is pretty equal distributed between ages. Since there are a lot of data could be difficult to see some relations, let's try to plot a random k-sample of the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3OJC0REwj8g3"
      },
      "source": [
        "# define k, the dimension of the sample\n",
        "k = 1000\n",
        "\n",
        "# get the k-dimensional sample\n",
        "sample1 <- sample(1:nrow(insurance.scaled), k)\n",
        "sample2 <- sample(1:nrow(insurance.scaled), k)\n",
        "sample3 <- sample(1:nrow(insurance.scaled), k)\n",
        "\n",
        "#plot the samples\n",
        "insurance.data[sample1, ] %>%\n",
        "    ggplot(aes(x = Age, y = Annual_Premium)) + \n",
        "    geom_point() +\n",
        "    geom_smooth(formula =  y ~ x, method = \"loess\")\n",
        "\n",
        "insurance.data[sample2, ] %>%\n",
        "    ggplot(aes(x = Age, y = Annual_Premium)) + \n",
        "    geom_point() +\n",
        "    geom_smooth(formula =  y ~ x, method = \"loess\")\n",
        "\n",
        "insurance.data[sample3, ] %>%\n",
        "    ggplot(aes(x = Age, y = Annual_Premium)) + \n",
        "    geom_point() +\n",
        "    geom_smooth(formula =  y ~ x, method = \"loess\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "34AjBudHj8g3"
      },
      "source": [
        "After we have plotted different samples nothing seems to appear, so we can think that the customer's age is not a determinant variable to the annual prize!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CHyTY-UIj8g3"
      },
      "source": [
        "### 4.5. Vehicle_Age~Annual_Premium\n",
        "\n",
        "Let's see how the vehicle age influence the annual premium. Since the Annual_Premium variable is a continuous variable with a really large range, we will delete the last part due to the fact that there are really low samples with an annual premium greather than $100,000\\$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gt2NSjb9j8g3"
      },
      "source": [
        "insurance.data %>%\n",
        "    ggplot(aes(x = Annual_Premium,fill = as.factor(Vehicle_Age), color = as.factor(Vehicle_Age))) +\n",
        "    geom_density(alpha = 0.3, size = 1) +\n",
        "    coord_cartesian(xlim = c(-1000,90000)) +\n",
        "    guides(color=guide_legend(title=\"Vehicle_Age\"), fill=guide_legend(title=\"Vehicle_Age\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IuF6NnIXj8g4"
      },
      "source": [
        "We can see that there are a lot of sample with the lower annual price, indipendently from the vehicle age, but leaving these \"marginal\" points we can see a slighty difference between the premium according to the vehicle ages, the premium increase at the incrasing of the age."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FrN8Fc_Zj8g4"
      },
      "source": [
        "### 4.6. Vehicle_Damage~Annual_Premium\n",
        "\n",
        "We expect that if a vehicle got damaged the annual premium increase, but let's see if this really happens"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1qCjUJFqj8g4"
      },
      "source": [
        "insurance.data %>%\n",
        "    ggplot(aes(x = Annual_Premium,color = Vehicle_Damage, fill = Vehicle_Damage)) +\n",
        "    geom_density(alpha = 0.3, size = 1) +\n",
        "    coord_cartesian(xlim = c(6000,100000))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "34qYKJvrj8g4"
      },
      "source": [
        "As we can see, the damage have not a significat weight on the calculation of the annual premium, if a car is damaged there are only few more probability to pay an higher premium. We can say that by looking at the small right shifting of the Damaged Vehicle distribution but in average people pay the same amount for the annual premium indipendently if the vehicle is damaged or not."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BmzRRekQj8g4"
      },
      "source": [
        "### 4.7. Vehicle_Age~Vehicle_Damage-Annual_Premium\n",
        "\n",
        "Since both Vehicle_Age and Vehicle_Damage variables seems to give a limited contribute to the annual prize, let's see how this two variables contribute together"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qvk7540Hj8g4"
      },
      "source": [
        "insurance.data %>%\n",
        "    ggplot(aes(x = Annual_Premium,color = as.factor(Vehicle_Age), fill = as.factor(Vehicle_Age))) +\n",
        "    geom_density(alpha = 0.3, size = 1) +\n",
        "    coord_cartesian(xlim = c(6000,100000)) +\n",
        "    facet_wrap(~Vehicle_Damage)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "acHdY8wDj8g4"
      },
      "source": [
        "Thanks to this plot we can see an interesting fact:\n",
        "\n",
        "- There are no old (>2 years; blue bell) cars without damages;\n",
        "\n",
        "- A person with an older car and with no damage is awarded in the annual premium rather than the youger car, instead if a person with an old car got his car damaged, then the annual premium become, in general, higher than the newest cars!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t4wCGnkoj8g4"
      },
      "source": [
        "## 5. Principal Components Analysis\n",
        "\n",
        "It is important to understand the variablity of the data, to do so we will apply the Principal Component Analysis in order to check out which variables explains more variablity.\n",
        "\n",
        "We are going to apply the PCA only on the numeric variables.\n",
        "\n",
        "Principal Component Analysis is a dimensionality reduction technique such that allows us to get a low dimensional representation of the dataset describing as much variability as possible.\n",
        "\n",
        "This operation takes a $n\\times d$ dataset and returns a $n\\times d$ dataset, but here we have that the last $k$ principal components (the variables in the new space) are less useful than the others so we can delete them without losing a lot of information.\n",
        "\n",
        "These components are obtained by applying the eigen decomposition on the original data and taking the eigen vectors. In other words the $i^{th}$ principal component is a linear combination of the original variables, with some weights defined by the $i^{th}$ eigen vector with the constraint that each PC has to be orthogonal with all the others.\n",
        "\n",
        "The PCA makes sense only if there is an high level of correlation between data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0l4DO9cXj8g4"
      },
      "source": [
        "# calculate pca\n",
        "pca <- prcomp(insurance.scaled[,num_cols], scale. = F, center = F)\n",
        "\n",
        "# show some informations\n",
        "(summary.pca <- summary(pca))\n",
        "\n",
        "pca"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xSo2ZJllj8g4"
      },
      "source": [
        "Thanks to this summary we can see that with the first 2 cover aproximately $90\\%$ of the variability!\n",
        "\n",
        "But let's analyze it graphically"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3OwdwyYmj8g4"
      },
      "source": [
        "plot(summary.pca$importance[3,], xlab = \"Principal Component\",\n",
        "     ylab = \"Cumulative Proportion of Variance Explained\",\n",
        "     type = \"b\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qShQyRhEj8g4"
      },
      "source": [
        "We will take only the first $2$ Principal components and leave all the others.\n",
        "\n",
        "Let's plot the dataset in this new set of variables."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wp-yJif2j8g4"
      },
      "source": [
        "fviz_pca_ind(pca, title = \"PCA\", legend = \"top\",\n",
        "geom = \"point\", ggtheme = theme_classic())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0fzfzOtjj8g4"
      },
      "source": [
        "Thanks to this plot we can see that, in the new space, there are at least two separate group of customers. The first group could be the one with $PC2 <= -0.5$ and the other points going into the second one\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pfitq8V4j8g4"
      },
      "source": [
        "We could also think to separate the data into 3 groups only based on the second Principal Component.\n",
        "Let's check out the contribute of each variable to each Principal Component."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lOBBijeaj8g4"
      },
      "source": [
        "var <- get_pca_var(pca)\n",
        "\n",
        "corrplot(t(var$contrib[,1:2]), is.corr=FALSE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OFnu8mvRj8g4"
      },
      "source": [
        "As we can see the second Principal Component is mainly determined by *Age*, *Vehicle_Age* and *Policy_Sales_Channel* and we know that this variables are correlated each other. Instead the first principal component is pretty equally composed by all the variables.\n",
        "\n",
        "Thanks to the plot above and the heatmap we have just discussed, we can see that the second Principal Component is pretty well clustered, but looking at the first principal component, which is composed by all the variables, we can see that the most part of the samples form some *clouds of points* but there are also some samples standing outside these clouds.\n",
        "\n",
        "Let's analyze the two Principal Components separately"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KUIAYle0j8g4"
      },
      "source": [
        "# get the first two principal components\n",
        "pc.data <- as.data.frame(pca$x[,1:2])\n",
        "\n",
        "pc.data %>% \n",
        "    ggplot(aes(PC1)) + \n",
        "    geom_density(color = \"darkblue\", fill = \"lightblue\") +\n",
        "    geom_vline(aes(xintercept=mean(PC1)), color = \"red\", linetype = \"dashed\", size = 1) +\n",
        "    ggtitle(\"Plot PC1\") +\n",
        "    theme(plot.title = element_text(size = 30))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XVStDrcDj8g4"
      },
      "source": [
        "As we saw in the scatterplot the first Principal Component follow a normal distribution with a little right skewness.\n",
        "\n",
        "Now it's time to look at the second Principal Component."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "38OAH29wj8g4"
      },
      "source": [
        "pc.data %>%\n",
        "    ggplot(aes(PC2)) +\n",
        "    geom_density(color = \"darkblue\", fill = \"lightblue\") +\n",
        "    geom_vline(aes(xintercept=mean(PC2)), color = \"red\", linetype = \"dashed\", size = 1) +\n",
        "    ggtitle(\"Plot PC2\") +\n",
        "    theme(plot.title = element_text(size = 30))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ET-dwnpwq-lS"
      },
      "source": [
        "Here we can see three different peaks, these are the three potential clusters we said before.\n",
        "\n",
        "Now let's try to give an other reppresentation of the data, showing to only the scatterplot in PCs space but also some indication about how the original variables behaviour on the PCs space"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_-02KpzGrSsG"
      },
      "source": [
        "autoplot(pca, data = insurance.scaled,\n",
        "         loadings = TRUE, col = \"lightgray\",loadings.colour = 'red',\n",
        "         loadings.label = TRUE, loadings.label.size = 3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NAhHYqvaj8g4"
      },
      "source": [
        "\n",
        "## 6. Outlier Detection\n",
        "\n",
        "It could be interesting to study the first principal component in order to check out if these outliers are there due to noise on the data or these are some interesting points.\n",
        "\n",
        "### 6.1 Hampel filter\n",
        "\n",
        "Here we will apply a smoother version of the Hampel filter to the first Principal Component hoping that this technique can identify the right samples.\n",
        "\n",
        "This is an outlier detection technique, it consists of considering as outliers the values outside the interval ($I$) formed by the median, plus or minus 3 times the median absolute deviations($MAD$): $$I = [median - 2.5\\cdot MAD; median-2.5\\cdot MAD]$$\n",
        "\n",
        "where MAD is the median absolute deviation and is defined as the median of the absolute deviations from the data’s median $\\tilde{X}=median(X)$ \n",
        "\n",
        "$$MAD = median(|X_i - \\tilde{X}|)$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f9QJ7nN0j8g4"
      },
      "source": [
        "# calculate the lower bound\n",
        "lower_bound <- median(pc.data$PC1) - 2.5 * mad(pc.data$PC1)\n",
        "\n",
        "# calculate the upper bound\n",
        "upper_bound <- median(pc.data$PC1) + 2.5 * mad(pc.data$PC1)\n",
        "\n",
        "# get the outliers indexes\n",
        "left_outlier_ind <- which(pca$x[,1] < lower_bound)\n",
        "right_outlier_ind <- which(pca$x[,1] > upper_bound)\n",
        "\n",
        "# creating a copy of the data\n",
        "hampel.data <- pc.data\n",
        "\n",
        "# set the sampler as outlier\n",
        "hampel.data$is.outlier <- 1:nrow(hampel.data)\n",
        "\n",
        "hampel.data$is.outlier[-left_outlier_ind] = 1 #no outliers\n",
        "hampel.data$is.outlier[left_outlier_ind] = 2 # < lower_bound\n",
        "hampel.data$is.outlier[right_outlier_ind] = 3 # > upper_bound\n",
        "\n",
        "hampel.data$is.outlier <- as.factor(hampel.data$is.outlier)\n",
        "\n",
        "# plot the new data\n",
        "colVar <- sapply(hampel.data$is.outlier, function(a){ifelse(a == 1,'red', ifelse(a == 2,'green', 'blue'))})\n",
        "colVar <- factor(colVar,levels=c('red','green','blue'))\n",
        "\n",
        "hampel.data %>% ggplot(aes(PC1,PC2, color = as.factor(is.outlier))) + geom_point()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s26zo6htj8g4"
      },
      "source": [
        "We caught most of the left side and right side outliers! Let's analyze both the groups."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DhSwRDl6j8g4"
      },
      "source": [
        "temp.data <- as.data.frame(insurance.scaled)\n",
        "temp.data$class <- hampel.data$is.outlier\n",
        "\n",
        "temp.data %>%\n",
        "    gather(variable, value, -class) %>%\n",
        "    ggplot(aes(y = as.factor(variable),\n",
        "              fill = class,\n",
        "              color = class,\n",
        "              x = percent_rank(value))) +\n",
        "    geom_density_ridges(alpha = 0.5) +\n",
        "    xlab(\"percent_rank\") +\n",
        "    ylab(\"variable\") +\n",
        "    scale_fill_discrete(name = \"Dose\", labels = c(\"No Outlier\", \"Left Outlier\", \"Right Outlier\")) +\n",
        "    scale_color_discrete(name = \"Dose\", labels = c(\"No Outlier\", \"Left Outlier\", \"Right Outlier\")) +\n",
        "    guides(fill=guide_legend(title=\"Class\"), color=guide_legend(title=\"Class\")) +\n",
        "    ggtitle(\"Plot\") +\n",
        "    theme(plot.title = element_text(size = 30))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EvwgBVh6j8g5"
      },
      "source": [
        "We can see that *Gender* and *Driving_License* are pretty equal in the three classes. We also have that the left outliers are, in general, old people who have been customers of the company for long time driving old cars and got offered a very high annual premium. The right outliers are exactly the opposite of the left outliers!\n",
        "\n",
        "The *Policy_Sales_Channel* and *Region_Code* seems to be two others crucial variables to the outliers.\n",
        "\n",
        "Let's check if our analysis matches the original outliers by plotting the PCs and coloring them by the response variable"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kTwMeXWYtTga"
      },
      "source": [
        "autoplot(pca, data = insurance.scaled,col = c(\"red\",\"green\")[response.var + 1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-gKVKqwduCr8"
      },
      "source": [
        "This technique doesn't allow us to find the right outliers because the two plots are really different."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8U5mnTw1j8g5"
      },
      "source": [
        "## 7. Cluster Analysis\n",
        "\n",
        "In this chapter we will carry out some significant clusters on the dataset (if any).\n",
        "\n",
        "Cluster Analysis is the most common task in unsupervised machine learning and it is a process aimed to check out if there are groups in the data related by some common characteristics.\n",
        "\n",
        "Here we will analyze three different clustering methods:\n",
        "  1. Hierarchical Clustering\n",
        "  2. Partitioning Clustering\n",
        "  3. Model based Clustering\n",
        "\n",
        "**NOTE:** the following discussions are really general and naive, i will explain them in a better and more correct way during the oral exam\n",
        "\n",
        "##### Hierarchical Clustering\n",
        "\n",
        "Given a measure of distance, a linkage criteria and a dataset this type of clustering works as follow:\n",
        "\n",
        "  1. Assign each sample to a different cluster\n",
        "  2. Merge the two closest clusters into a single bigger cluster according to the choosen distance and linkage criteria\n",
        "  3. repeat point $2$ until all samples are in the same cluster\n",
        "\n",
        "NOTE: there is also another way to apply hierarchical clustering that works in exactly opposite way.\n",
        "\n",
        "##### Partitioning Clustering\n",
        "\n",
        "Given a measure of distance and the number of clusters $k$ this type of clustering works as follow:\n",
        "\n",
        "  1. randomly choice $k$ different samples as cluster representatives\n",
        "  2. assign each sample to the closest cluster\n",
        "  3. generate new representatives for the clusters based on the new disposition\n",
        "  4. repeat $3$ until convergence\n",
        "\n",
        "##### Model based Clustering\n",
        "\n",
        "In this type of clustering the main idea is that each cluster is fitted by a different set of distributions, so we have to choice the best distributions to reppresent our data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "crtyyr1dj8g5"
      },
      "source": [
        "### 7.1 Clustering tendency\n",
        "\n",
        "Before going on the cluster analysis we have to assert that the dataset contains meaningful clusters or not. To do that we will use the hopkins statistic and the VAT Algorithm for a graphical representation.\n",
        "\n",
        "#### 7.1.1 VAT Visual Assessment of cluster Tendency\n",
        "\n",
        "First of all we will analyze the VAT results.\n",
        "\n",
        "VAT is a visual process to find cluster tendency in the data, all the samples being ordered based on their distance between each other, se samples that are closer in the VAT plot are closer in the real space."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jdcf7mP7j8g5"
      },
      "source": [
        "# calculate the gower distance matrix\n",
        "dist <- as.dist(gower.dist(insurance.scaled))\n",
        "\n",
        "# VAT\n",
        "fviz_dist(dist, show_labels = F, gradient = list(low = \"black\", mid = \"white\", high = \"white\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-4uNfT1Aj8g5"
      },
      "source": [
        "The VAT algorithm shows up that the data seems pretty well clustered since there are some *large squares* into the first diagonal of the VAT plot.\n",
        "\n",
        "#### 7.1.2 Hopkins Statistic\n",
        "\n",
        "Even if the VAT told us that the dataset was clustered we will apply a statistical test, the **Hopkins Statistic** to have a numerical value of the cluster tendency of the dataset.\n",
        "The Hopkins Statistic is a statistical way to measure the cluster tendency of a dataset, it will take values in $[0,1]$. Since the dataset is composed by categorical and numeric variables we have to use the gower distance, hence we cannot apply the pre-built R function for the Hopkins statistic but we have to create it by scratch. \n",
        " \n",
        " First of all we will define all the functions which allow us to compute the Hopkins Statistic."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l8tZMx3Tvawc"
      },
      "source": [
        "# function to create the sampled dataset\n",
        "create.sampled.data <- function(m, X){\n",
        "  Y.indexs <- sample(1:nrow(X), m)\n",
        "  Y <- X[Y.indexs,]\n",
        "}\n",
        "\n",
        "# function to create the random artificial dataset\n",
        "create.artificial.data <- function(m, X){\n",
        "  Gender <- as.factor(sample(c(\"Male\", \"Female\"), m, replace = T, prob = c(0.5,0.5)))\n",
        "  Age <- runif(m, min = min(X$Age), max = max(X$Age))\n",
        "  Driving_License <- as.factor(sample(c(\"Yes\", \"No\"), m, replace = T, prob = c(0.5,0.5)))\n",
        "  Region_Code <- runif(m, min = min(X$Region_Code), max = max(X$Region_Code))\n",
        "  Previously_Insured <- as.factor(sample(c(\"Yes\", \"No\"), m, replace = T, prob = c(0.5,0.5)))\n",
        "  Vehicle_Age <- runif(m, min = min(X$Vehicle_Age), max = max(X$Vehicle_Age))\n",
        "  Vehicle_Damage <- as.factor(sample(c(\"Yes\", \"No\"), m, replace = T, prob = c(0.5,0.5)))\n",
        "  Annual_Premium <- runif(m, min = min(X$Annual_Premium), max = max(X$Annual_Premium))\n",
        "  Policy_Sales_Channel <- runif(m, min = min(X$Policy_Sales_Channel), max = max(X$Policy_Sales_Channel))\n",
        "  Vintage <- runif(m, min = min(X$Vintage), max = max(X$Vintage))\n",
        "\n",
        "  # put all together in a dataset\n",
        "  Z <- data.frame(Gender, Age, Driving_License, Region_Code, Previously_Insured, Vehicle_Age, Vehicle_Damage, Annual_Premium, Policy_Sales_Channel, Vintage)\n",
        "  return(Z)\n",
        "}\n",
        "\n",
        "# function to calculate distances\n",
        "calc.dist <- function(X, Y, m, is.original = T){\n",
        "  YX.dists <- 0\n",
        "  if(is.original == T){\n",
        "    for(i in 1:m){\n",
        "      # X[rownames(X) != rownames(Y[i,])[1],] allows us to avoid the distance\n",
        "      #                                       between a row with itself\n",
        "      YX.dists <- YX.dists + min(gower.dist(Y[i,], X[rownames(X) != rownames(Y[i,])[1],]))\n",
        "    }\n",
        "  }\n",
        "  else {\n",
        "    for(i in 1:m){\n",
        "      YX.dists <- YX.dists + min(gower.dist(Y[i,], X))\n",
        "    }\n",
        "  }\n",
        "  return(YX.dists)\n",
        "}\n",
        "\n",
        "# function to calculate hopkins statistic\n",
        "my.hopkins <- function(X, m){\n",
        "  Y <- create.sampled.data(m, X)\n",
        "  Z <- create.artificial.data(m, X)\n",
        "  YX.dist <- calc.dist(X, Y, m, T)\n",
        "  ZX.dist <- calc.dist(X, Z, m, F)\n",
        "  H <- YX.dist / (ZX.dist + YX.dist)\n",
        "  \n",
        "  return(H) \n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FRMgfwl7AlYi"
      },
      "source": [
        "Now we will apply the Hopkins statistic to our dataset with $m = 4000$ (we will generate the two datasets with $m$ rows)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X4pRqyJiyy42"
      },
      "source": [
        "m <- 4000\n",
        "my.hopkins(insurance.scaled, m)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hQoO9P9pVnjS"
      },
      "source": [
        "We got a really nice value for the test, $0.1$, it means that the dataset is well clusterable and our discussions about the VAT algorithm were right! "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jg5P9BPTj8g5"
      },
      "source": [
        "### 7.2 Optimal number of clusters & Best clustering algorithm\n",
        "\n",
        "Now that we know that the dataset is clusterable we have to check out the best number of clusters.\n",
        "We will compute some different clustering algorithm and evaluatig some plots and statistics in order to check out the best combination: **Number of clusters ~ Algorithm**.\n",
        "\n",
        "We will analyze the following Clustering Algorithms:\n",
        "  * PAM\n",
        "  * Hierarchical (with different configurations)\n",
        "\n",
        "#### 7.2.1 Optimal number of clusters\n",
        "\n",
        "Here we will analyze the results of the clustering algorithms in order to identify the optimal number of clusters\n",
        "\n",
        "In order to use the GAP statistic we have to build our function, because the pre-built function doesn't allow categorical data.\n",
        "\n",
        "Let's write the functions to compute *WSS*, *Elbow Method*,  and *GAP Statistic* respectively.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sglZgQWVj8g5"
      },
      "source": [
        "# elbow function\n",
        "elbow <- function(diss, alg = \"pam\", max.nc = 6, method = NULL){\n",
        "  elbow <- c()\n",
        "  title <- \"\"\n",
        "  if (alg == \"pam\"){\n",
        "    for (i in 1:max.nc){\n",
        "      fit <- pam(diss, diss = T, k = i)\n",
        "      stats <-  cluster.stats(diss ,fit$clustering,silhouette = F,\n",
        "                                sepindex = F, sepwithnoise = F)\n",
        "      elbow[i] <- stats$within.cluster.ss\n",
        "    }\n",
        "    title <- paste(alg, \"WSS Plot\", sep = \" \")\n",
        "  }\n",
        "  else{\n",
        "    fit <- hclust(diss, method)\n",
        "    for (i in 1:max.nc){\n",
        "      stats <- cluster.stats(diss, cutree(fit, k = i),silhouette = F,\n",
        "                             sepindex = F, sepwithnoise = F)\n",
        "      elbow[i] <- stats$within.cluster.ss\n",
        "    }\n",
        "    title <- paste(alg, method, \"WSS Plot\", sep = \" \")\n",
        "  }\n",
        "\n",
        "  plot(1:max.nc, elbow, \n",
        "    xlab = \"Number of clusters\",\n",
        "    ylab = \"WSS\",\n",
        "    main = title)\n",
        "  lines(1:max.nc, elbow)\n",
        "}\n",
        "\n",
        "takeBestGap <- function(K, wss, gap){\n",
        "  B <- 500 #the typical value of bootstrap\n",
        "  for(k in 1:(K-1)){\n",
        "    sd <- 0\n",
        "    mean <- 0\n",
        "    mean <- mean + log(wss[k + 1])\n",
        "    mean <- 1/B * mean\n",
        "    intern <- 0\n",
        "    intern <- intern + ((wss[k + 1] - mean ) ** 2)\n",
        "    \n",
        "    sd <- sqrt( 1/B * intern )\n",
        "    s <- sqrt(1 + 1/B) * sd\n",
        "    if (gap[k] >= gap[k+1] - s) {\n",
        "      return(k)\n",
        "    }\n",
        "  }\n",
        "  return(K)\n",
        "}\n",
        "\n",
        "\n",
        "gap <- function(diss, alg = \"pam\", max.nc = 6, method = \"/\", plot = F){\n",
        "  gap <- c()\n",
        "  wss <- c()\n",
        "  title <- \"\"\n",
        "   if (alg == \"pam\"){\n",
        "    for (i in 1:max.nc){\n",
        "      fit <- pam(x = diss, diss = T, k = i)\n",
        "      clusters <- fit$clustering\n",
        "      stats <- cluster.stats(diss, clusters, silhouette = F, sepindex = F, sepwithnoise = F)\n",
        "      gap[i] <- stats$widestgap\n",
        "      wss[i] <- stats$average.within\n",
        "    }\n",
        "    title <- paste(alg, \"GAP Plot\", sep = \" \")\n",
        "  }\n",
        "  else{\n",
        "    fit <- hclust(diss, method)\n",
        "    for (i in 1:max.nc){\n",
        "      stats <- cluster.stats(diss, cutree(fit, k = i), silhouette = F,\n",
        "                             sepindex = F, sepwithnoise = F)\n",
        "      gap[i] <- stats$widestgap\n",
        "      wss[i] <- stats$average.within\n",
        "    }\n",
        "    title <- paste(alg, method, \"GAP Plot\", sep = \" \")\n",
        "  }\n",
        "  if(plot == T){\n",
        "    plot(1:max.nc, gap, #stats#stats\n",
        "\n",
        "\n",
        "      xlab = \"Number of clusters\",\n",
        "      ylab = \"GAP\",\n",
        "      main = title)\n",
        "    lines(1:max.nc, gap)\n",
        "  }\n",
        "  best.nclust <- takeBestGap(max.nc, wss, gap)\n",
        "  if(alg == \"hclust\") {alg <- \"hierarchical\"}\n",
        "  row <- data.frame(row.names = \"GAP\", Score = gap[best.nclust], Method = alg, Clusters = best.nclust)\n",
        "  return(row)\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "getClusStats <- function(data, dist, max.nc){\n",
        "  pam.clvalid <- myClValid.cat(data, nClust = 2:max.nc, clMethods = \"pam\",\n",
        "                               validation = c(\"internal\", \"stability\"),\n",
        "                               maxitems = nrow(data) + 1)\n",
        "  pam.gap <- gap(diss = dist, alg = \"pam\", max.nc = max.nc)\n",
        "\n",
        "  h.ward.D <- myClValid.cat(data, nClust = 2:max.nc, clMethods = \"hierarchical\",\n",
        "                            validation = c(\"internal\", \"stability\"),\n",
        "                            maxitems = nrow(data) + 1,\n",
        "                            method = \"ward.D\")\n",
        "  h.wd.gap <- gap(diss = dist, alg = \"hclust\", method = \"ward.D\", max.nc = max.nc)\n",
        "\n",
        "\n",
        "  h.ward.D2 <-myClValid.cat(data, nClust = 2:max.nc, clMethods = \"hierarchical\",\n",
        "                            validation = c(\"internal\", \"stability\"),\n",
        "                            maxitems = nrow(data) + 1,\n",
        "                            method = \"ward.D2\")\n",
        "  h.wd2.gap <- gap(diss = dist, alg = \"hclust\", method = \"ward.D2\", max.nc = max.nc)\n",
        "\n",
        "\n",
        "  h.single <- myClValid.cat(data, nClust = 2:max.nc, clMethods = \"hierarchical\",\n",
        "                            validation = c(\"internal\", \"stability\"),\n",
        "                            maxitems = nrow(data) + 1,\n",
        "                            method = \"single\")\n",
        "  h.single.gap <- gap(diss = dist, alg = \"hclust\", method = \"single\", max.nc = max.nc)\n",
        "\n",
        "\n",
        "  h.complete <- myClValid.cat(data, nClust = 2:max.nc, clMethods = \"hierarchical\",\n",
        "                              validation = c(\"internal\", \"stability\"),\n",
        "                              maxitems = nrow(data) + 1,\n",
        "                              method = \"complete\")\n",
        "  h.complete.gap <- gap(diss = dist, alg = \"hclust\", method = \"complete\", max.nc = max.nc)\n",
        "\n",
        "\n",
        "  h.average <- myClValid.cat(data, nClust = 2:max.nc, clMethods = \"hierarchical\",\n",
        "                             validation = c(\"internal\", \"stability\"),\n",
        "                             maxitems = nrow(data) + 1,\n",
        "                             method = \"average\")\n",
        "  h.average.gap <- gap(diss = dist, alg = \"hclust\", method = \"average\", max.nc = max.nc)\n",
        "\n",
        "  pam.clvalid <- optimalScores(pam.clvalid)\n",
        "  h.ward.D <- optimalScores(h.ward.D)\n",
        "  h.ward.D2 <- optimalScores(h.ward.D2)\n",
        "  h.single <- optimalScores(h.single)\n",
        "  h.complete <- optimalScores(h.complete)\n",
        "  h.average <- optimalScores(h.average)\n",
        "\n",
        "  pam.clvalid <- rbind(pam.clvalid, pam.gap)\n",
        "  h.ward.D <- rbind(h.ward.D, h.wd.gap)\n",
        "  h.ward.D2 <- rbind(h.ward.D2, h.wd2.gap)\n",
        "  h.single <- rbind(h.single, h.single.gap)\n",
        "  h.complete <- rbind(h.complete, h.complete.gap)\n",
        "  h.average <- rbind(h.average, h.average.gap)\n",
        "\n",
        "\n",
        "  optimals <- pam.clvalid\n",
        "  optimals <- rbind(optimals, h.ward.D,h.ward.D2,h.single,h.complete,h.average)\n",
        "  rownames(optimals) <- 1:nrow(optimals)\n",
        "\n",
        "  Metric <- append(rownames(pam.clvalid), append(rownames(h.ward.D), append(rownames(h.ward.D2),\n",
        "                   append(rownames(h.single), append(rownames(h.complete), rownames(h.average))))))\n",
        "  Type <- c(\"/\", \"/\", \"/\",\"/\",\"/\",\"/\",\"/\",\"/\",\n",
        "              \"ward.D\",\"ward.D\",\"ward.D\",\"ward.D\",\"ward.D\",\"ward.D\",\"ward.D\",\"ward.D\",\n",
        "              \"ward.D2\", \"ward.D2\", \"ward.D2\", \"ward.D2\", \"ward.D2\", \"ward.D2\", \"ward.D2\",\"ward.D2\",\n",
        "              \"single\",\"single\",\"single\",\"single\",\"single\",\"single\",\"single\",\"single\",\n",
        "              \"complete\",\"complete\",\"complete\",\"complete\",\"complete\",\"complete\",\"complete\",\"complete\",\n",
        "              \"average\",\"average\",\"average\",\"average\",\"average\",\"average\",\"average\",\"average\")\n",
        "  optimals <- cbind(Metric, optimals[,1:2,drop=F], Type, optimals[,3,drop=F])\n",
        "  return(optimals)\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tDK8NCLzdSbH"
      },
      "source": [
        "Now that we have defined our functions let's check the best number of clusters!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H5wTjo71dZjP"
      },
      "source": [
        "clustats <- getClusStats(data = insurance.scaled, dist = dist, max.nc = 6)\n",
        "head(clustats)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HDuMgeJEwuz4"
      },
      "source": [
        "Now we could identify the optimal number of clusters using the majority rule by doing a little operation on the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c36iwbSWytwW"
      },
      "source": [
        "clustats %>% \n",
        "  group_by(Clusters) %>%\n",
        "  summarise(number = n()) %>%\n",
        "  arrange(desc(number))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ftqgKXeJdyu8"
      },
      "source": [
        "According to the majority rule the best number of clusters is $K = 2$. But let's check also what the elbow method says.\n",
        "\n",
        "Here we will plot the elbow method for each algorithm!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ulGU9JsUa9uQ"
      },
      "source": [
        "# PAM\n",
        "elbow(diss = dist, alg = \"pam\", max.nc = 6)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yi_134qqQnlc"
      },
      "source": [
        "We can see two different elbows in the plot regarding the PAM algorithm, one at 2 clusters and an other on 4.\n",
        "Since the elbow at 4 clusters lend to a stronger change into the plot we will take the optimal number of clusters according to this elbow method is $K_{pam} = 4$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TYVxZeoSQmLI"
      },
      "source": [
        "# hclust with ward.D linkage method\n",
        "elbow(diss = dist, alg = \"hclust\", method = \"ward.D\", max.nc = 6)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vn8EQqW7QpVx"
      },
      "source": [
        "Also in this plot the previous consideration could be done, so we will take $K_{ward.D} = 4$ as the optimal number of clusters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JqrG_jLiQq-4"
      },
      "source": [
        "# hclust with ward.D2 linkage method\n",
        "elbow(diss = dist, alg = \"hclust\", method = \"ward.D2\", max.nc = 6)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7vviYBkDQrUT"
      },
      "source": [
        "In this plot we have one single elbow at 2 so we will take $K_{ward.D2} = 2$ as the optimal number of clusters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xofUFh2nQsPX"
      },
      "source": [
        "# hclust with single linkage method\n",
        "elbow(diss = dist, alg = \"hclust\", method = \"single\", max.nc = 6)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8fAI0FhJQsgi"
      },
      "source": [
        "Something strange seems to appear using the single linkage method, so, we will not consider this method for the elbow considerations. We will set $K_{single} = \\pm \\infty $"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3NBPehzBQs9h"
      },
      "source": [
        "# hclust with complete linkage method\n",
        "elbow(diss = dist, alg = \"hclust\", method = \"complete\", max.nc = 6)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZGt9Wc_XQtP8"
      },
      "source": [
        "This plot seems to be pretty strange, but we can see a different change of the curve at $K_{complete} = 3$ "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iha6PjFHQtmJ"
      },
      "source": [
        "# hclust with average linkage method\n",
        "elbow(diss = dist, alg = \"hclust\", method = \"average\", max.nc = 6)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w4TsBALPrAgc"
      },
      "source": [
        "This plot is similar to the previous one, so we are going to set $K_{average} = 3$\n",
        "\n",
        "Let's resume the elbow results in a table.\n",
        "\n",
        "## <center> Elbow results </center>\n",
        "\n",
        "|#| Method | Type | Clusters |\n",
        "|:-| :- | -: | :-: |\n",
        "|1| PAM | / | 4 |\n",
        "|2| Hierarchical | Ward.D | 4 |\n",
        "|3| Hierarchical | Ward.D2 | 2 |\n",
        "|4| Hierarchical | Single | $\\pm \\infty$ |\n",
        "|5| Hierarchical | Complete | 3 |\n",
        "|6| Hierarchical | Average | 3 |\n",
        "\n",
        "After all the analysis we can assert that the optimal number of clusters is $K = 2$.\n",
        "\n",
        "#### 7.2.2 Best clustering algorithm\n",
        "\n",
        "After having asserted that the best number of clusters is $K = 2$ we have to identify the best clustering algorithm. To do that we could use the dataset we made up earlier."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vM7gzyfEwICp"
      },
      "source": [
        "head(clustats)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ej9D-IV2-e2D"
      },
      "source": [
        "Now we have to do some operations in order to check out the best algorithm.\n",
        "\n",
        "First of all we will create a supplementary dataset containing the best algorithm for each metric with the constraint of 2 clusters.\n",
        "\n",
        "We wanna maximize the following metrics:\n",
        "  * Silhouette\n",
        "  * Dunn index\n",
        "  * Gap statistic\n",
        "\n",
        "and minimize these ones:\n",
        "  * APN\n",
        "  * AD\n",
        "  * ADM\n",
        "  * FOM\n",
        "  * Connectivity"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "13R-JNWVzgcc"
      },
      "source": [
        "stats <- data.frame(metric = character(), Score = double(), Method = character(),\n",
        "                    Type = character(), Clusters = integer())\n",
        "for (m in unique(clustats$metric)){\n",
        "  if (m %in% c(\"Silhouette\", \"Dunn\", \"GAP\")){\n",
        "    part <- clustats %>% filter(metric == m & Clusters == 2)\n",
        "    stats <- rbind(stats,part[which.max(part$Score),])\n",
        "  }\n",
        "  else{\n",
        "    part <- clustats %>% filter(metric == m & Clusters == 2)\n",
        "    stats <- rbind(stats,part[which.min(part$Score),])\n",
        "  }\n",
        "  \n",
        "}\n",
        "stats"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RNze421m_yL0"
      },
      "source": [
        "As we can see some metric are missing, this is due to the fact that no optimal scores contain the oprimal number of clusters $K = 2$ so the two missing metrics have been deleted from the data frame.\n",
        "\n",
        "To identify the optimal algorithm we will apply a little transformation on the data frame in order to make our decision using the majority rule.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YR2-4O2J6Vsy"
      },
      "source": [
        "stats %>% \n",
        "  group_by(Method, Type) %>%\n",
        "  summarise(number = n()) %>%\n",
        "  arrange(desc(number))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VoQfy5xs4PT9"
      },
      "source": [
        "From the table above we can assert that the optimal clustering algorithm for our dataset is the **hierarchical clustering** with the **single linkage** method.\n",
        "\n",
        "### 7.3 External clustering validation\n",
        "\n",
        "As we noticed at the beginning of the report, the dataset has a variable containing a label that can be used to validate clustering algorithms.\n",
        "\n",
        "Let's analyze the variable"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "23XtGqTgko8W"
      },
      "source": [
        "str(response.var)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U-UftrdHpz8P"
      },
      "source": [
        "This variable act like a factor so let's make it a factorial variable"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jaduRAkkp5hW"
      },
      "source": [
        "response.var <- as.factor(response.var)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zigk3rb2p7zL"
      },
      "source": [
        "Now we can plot the variable"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JbUbryNZp-qX"
      },
      "source": [
        "histogram(response.var, type = \"density\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0kdsvf3OrEQq"
      },
      "source": [
        "We can see that the dataset is pretty unbalanced with the $80\\%$ of the data is not interested in a healt insurance, this could be seen in the inverse problem of *chunk analysis*. \n",
        "\n",
        "Hence, clustering approaches could not be the best way to identify this kind of information about the data, but let's try to check if there is any clustering algorithm working well in that case.\n",
        "\n",
        "To do that we will use some two interesting indexes which analyze how well a clustering algorithm works like a \"classification algorithm\".\n",
        "\n",
        "These two indexes are:\n",
        "  * Corrected Rand Index --> maximized\n",
        "  * Meila's variation index (VI) --> minimized\n",
        "\n",
        "First of all let's create a data frame containing informations about the results, it will be used at the end in order to get the best approach to solve the task."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pkKzRHQY0djU"
      },
      "source": [
        "valuations <- data.frame(Method = character(),Type = character(),\n",
        "                         C.R.I. = numeric(), VI = numeric())\n",
        "\n",
        "str(valuations)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ruwdHUkD0dwm"
      },
      "source": [
        "#### 7.3.1 Partitioning Around Medoid (PAM)\n",
        "\n",
        "First of all we will valuate the PAM algorithm using the confusion matrix generated by a comparing between the original labels and the predicted ones"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jsP_xVHqqZTP"
      },
      "source": [
        "# computing the pam clustering\n",
        "fit <- pam(dist, diss = T, k = 2)\n",
        "\n",
        "# generating the confusion matrix\n",
        "table(response.var, fit$clustering)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S7JXxTxxvuaX"
      },
      "source": [
        "Here we have on the rows the original labels and on the columns the predicted ones and in each cell of the matrix we have how many samples of class $i$ has been classified as class $j$.\n",
        "\n",
        "We can see that the algorithm does a really bad work, it has just divied the data in the two classes in equal way. Hence we will expect that the two indexes give us bad results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zfn_4JcTwgQF"
      },
      "source": [
        "# validate clusters\n",
        "results <- cluster.stats(dist, fit$clustering, alt.clustering = as.numeric(response.var))\n",
        "\n",
        "# print results\n",
        "print(paste0(\"corrected rand index = \", results$corrected.rand))\n",
        "print(paste0(\"Meila's variation index (VI) = \", results$vi))\n",
        "\n",
        "# saving results\n",
        "pam.res <- data.frame(Method = \"PAM\", Type = \"/\",\n",
        "                      C.R.I. = results$corrected.rand, VI = results$vi)\n",
        "valuations <- rbind(valuations, pam.res)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hM4L2cjOx13u"
      },
      "source": [
        "We were right, bad values for the two indexes.\n",
        "\n",
        "#### 7.3.2 Hierarchical Clustering\n",
        "\n",
        "Here we will apply the same analysis on all the hierarchical clustering methods we saw above.\n",
        "\n",
        "##### Ward.D Linkage Method\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aqJ8wET-zj8-"
      },
      "source": [
        "# computing the hclust clustering\n",
        "fit <- hclust(dist, method = \"ward.D\")\n",
        "\n",
        "# cutting the tree\n",
        "clusters <- cutree(fit, 2)\n",
        "\n",
        "# generating the confusion matrix\n",
        "table(response.var, clusters)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qiNNALCA0T47"
      },
      "source": [
        "This method seems to work slightly better than PAM."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MiVPJAku2NvC"
      },
      "source": [
        "# validate clusters\n",
        "results <- cluster.stats(dist, clusters, alt.clustering = as.numeric(response.var))\n",
        "\n",
        "# print results\n",
        "print(paste0(\"corrected rand index = \", results$corrected.rand))\n",
        "print(paste0(\"Meila's variation index (VI) = \", results$vi))\n",
        "\n",
        "# saving results\n",
        "res <- data.frame(Method = \"Hierarchical\", Type = \"ward.D\",\n",
        "                      C.R.I. = results$corrected.rand, VI = results$vi)\n",
        "valuations <- rbind(valuations, res)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c1n6ah3X48_P"
      },
      "source": [
        "Also this method works really bad for our taks, but it was better than the previous one!\n",
        "\n",
        "##### Ward.D2 Linkage Method\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lX-S7ytl5Nn6"
      },
      "source": [
        "# computing the hclust clustering\n",
        "fit <- hclust(dist, method = \"ward.D2\")\n",
        "\n",
        "# cutting the tree\n",
        "clusters <- cutree(fit, 2)\n",
        "\n",
        "# generating the confusion matrix\n",
        "table(response.var, clusters)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7tjEs_LN5Prn"
      },
      "source": [
        "This algorithm also seems to work really bad on our data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e7NaX6QTkXk8"
      },
      "source": [
        "# validate clusters\n",
        "results <- cluster.stats(dist, clusters, alt.clustering = as.numeric(response.var))\n",
        "\n",
        "# print results\n",
        "print(paste0(\"corrected rand index = \", results$corrected.rand))\n",
        "print(paste0(\"Meila's variation index (VI) = \", results$vi))\n",
        "\n",
        "# saving results\n",
        "res <- data.frame(Method = \"Hierarchical\", Type = \"ward.D2\",\n",
        "                      C.R.I. = results$corrected.rand, VI = results$vi)\n",
        "valuations <- rbind(valuations, res)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ETzrYcAkifS"
      },
      "source": [
        "This works even worse than the previous one!\n",
        "\n",
        "###### Single Linkage Method"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ymM8t6O5kvq0"
      },
      "source": [
        "# computing the  hclust clustering\n",
        "fit <- hclust(dist, method = \"single\")\n",
        "\n",
        "# cutting the tree\n",
        "clusters <- cutree(fit, 2)\n",
        "\n",
        "# generating the confusion matrix\n",
        "table(response.var, clusters)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pjf9pOvhk8KT"
      },
      "source": [
        "This is a particular outcome, looking at the table all the points that are related to cluster 0 have been assigned correctly, but all the points related to cluster 1 have been misclassified!\n",
        "\n",
        "In this case the two indexes could give nice results, but looking at the confusion matrix we know that the model works really bad!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n0haxMR9k8lY"
      },
      "source": [
        "# validate clusters\n",
        "results <- cluster.stats(dist, clusters, alt.clustering = as.numeric(response.var))\n",
        "\n",
        "# print results\n",
        "print(paste0(\"corrected rand index = \", results$corrected.rand))\n",
        "print(paste0(\"Meila's variation index (VI) = \", results$vi))\n",
        "\n",
        "# saving results\n",
        "res <- data.frame(Method = \"Hierarchical\", Type = \"single\",\n",
        "                      C.R.I. = results$corrected.rand, VI = results$vi)\n",
        "valuations <- rbind(valuations, res)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r2ugpUKcp4ei"
      },
      "source": [
        "We got a really bad index for the CRI but a good value (compared with the others) for VI. \n",
        "\n",
        "To avoid errors we are going to delete this row from the dataset!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uJsmWJd9rAm-"
      },
      "source": [
        "valuations <- valuations[-nrow(valuations),]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8IIlgEJUrOfO"
      },
      "source": [
        "##### Complete Linkage Method"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s71ZWeAhrtZP"
      },
      "source": [
        "# computing the  hclust clustering\n",
        "fit <- hclust(dist, method = \"complete\")\n",
        "\n",
        "# cutting the tree\n",
        "clusters <- cutree(fit, 2)\n",
        "\n",
        "# generating the confusion matrix\n",
        "table(response.var, clusters)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UOPfvTOgr7BI"
      },
      "source": [
        "The complete linkage method gives more or less the same result of the single linkage, so we will obtain the same indexes and it makes no sense continue the computation.\n",
        "\n",
        "##### Average Linkage Method"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JK4NWLEpr7er"
      },
      "source": [
        "# computing the  hclust clustering\n",
        "fit <- hclust(dist, method = \"average\")\n",
        "\n",
        "# cutting the tree\n",
        "clusters <- cutree(fit, 2)\n",
        "\n",
        "# generating the confusion matrix\n",
        "table(response.var, clusters)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ql02rJO8s4Sg"
      },
      "source": [
        "We got the same result again.\n",
        "\n",
        "Let's check the data frame we made up above"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tLQaQ9shtOBN"
      },
      "source": [
        "valuations"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kqbyoT5dtUZ9"
      },
      "source": [
        "In conclusion we have that no one clustering algorithm helps us to identify the *not chunks* in our data set!\n",
        "\n",
        "### 7.4 Model Based Clustering\n",
        "\n",
        "In this last part of the report we will apply the model based clustering in our dataset.\n",
        "\n",
        "Model based clustering is a different approach to the clustering problem, it is based on the idea that if there are different clusters in the data, the samples in a cluster follow the same distribution, so our assumption is that if there are $K$ clusters there are $K$ different distributions in our data.\n",
        "\n",
        "According to what we said above, we will have $K$ different probability (mass) functions describing the clusters and the general distribution of all our dataset is a mixture of the $K$ different ones.\n",
        "\n",
        "We will apply the **Gaussian mixture model** algorithm which allows us to *read* a lot of possibile distributions in the data, both unimodal and multimodal by a combinations of multiple normal distributions!\n",
        "\n",
        "Since this type of clustering have to deal with a lot of parameters the computation could became infasible, so we have to check out the best trashold between accuracy and parsimony for our algorithm.\n",
        "\n",
        "Obviously this type of clustering works only on numeric variables, hence we will apply this algorithm only on the numeric variables of our dataset!\n",
        "\n",
        "NOTE: We are going to use the same dataset we used for [PCA](#5.-Principal-Components-Analysis)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gPz2T9Dr5wtB"
      },
      "source": [
        "# creating the sub-dataset\n",
        "numeric.df <- insurance.scaled[,num_cols]\n",
        "\n",
        "# applying the parsimonious Gaussian Mixture Model\n",
        "mod <- Mclust(numeric.df, G = 1:6, modelNames = NULL)\n",
        "\n",
        "# Plot BIC values for all the fitted models\n",
        "plot(mod, what = \"BIC\", ylim = range(mod$BIC, na.rm = TRUE),\n",
        "     legendArgs = list(x = \"topleft\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TRyIAWKr-FF6"
      },
      "source": [
        "As we can see the best model checkout 5 different clusters, we also have that the best model has volume variable for each cluster, equal shapes and variable orientations!\n",
        "\n",
        "Since in the graph is pretty difficult to quantify the differences between models we will show the BIC values for the top 3 best models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m2re14Er-WM1"
      },
      "source": [
        "summary(mod$BIC)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DWpGTbpcbD22"
      },
      "source": [
        "As we can see there is a significant difference between the first and the second model, but does the first model classifies in a good way the data?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wMaIuihABZB4"
      },
      "source": [
        "pairs(numeric.df, gap=0, pch = 16, col = mod$classification)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hpzj9YuZecuX"
      },
      "source": [
        "We can see that the model works pretty well for the *Policy_Sales_Channel* but it was really bad on most of the other variables!\n",
        "\n",
        "Let's check how it works on overall by visualizing the result in PC space"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fSARJ3pDewAL"
      },
      "source": [
        "fviz_mclust(mod, \"classification\", geom = \"point\", pointsize = 1.5, palette = \"jc\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yOAfbpROgjSn"
      },
      "source": [
        "As we say before there is a lot of confusion in this plot. The first and five-th clusters seems to have sense, but the other ones are all concentrated in a portion of data that is clearly one single cluster! \n",
        "\n",
        "It could be interesting to see the level of *self confidence* the model has"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nOo9_v9ZhCJ1"
      },
      "source": [
        "fviz_mclust(mod, \"uncertainty\", palette = \"jc\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "48NqHNrLicNJ"
      },
      "source": [
        "As we could imagine the model is hard sure abount first and five-th clusters, but it has a lot of uncertainty about all the other clusters!\n",
        "\n",
        "Only for curiosity let's see if only using the numerical variable and the parsimonius model we can extract the correct pattern on the data to match our chunk analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y76hM2mUkZ3m"
      },
      "source": [
        "mod.two <- Mclust(numeric.df, G = 2, modelNames = NULL)\n",
        "\n",
        "table(response.var, mod.two$classification)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "69AI04RXkoSP"
      },
      "source": [
        "Also this model works really bad to our task.\n",
        "\n",
        "## 8. Conclusion\n",
        "\n",
        "In order to solve our chunk detection task we tried a lot of different variantions like PCA analysis, euristic based clustering and model based clustering and all of these stuffs worked in bad way!\n",
        "\n",
        "So we could think that solving that task with an unsupervised approach was a really bad idea!"
      ]
    }
  ]
}