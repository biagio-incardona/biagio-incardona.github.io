{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "celltoolbar": "Edit Metadata",
    "kernelspec": {
      "display_name": "R",
      "language": "R",
      "name": "ir"
    },
    "language_info": {
      "codemirror_mode": "r",
      "file_extension": ".r",
      "mimetype": "text/x-r-source",
      "name": "R",
      "pygments_lexer": "r",
      "version": "3.6.3"
    },
    "colab": {
      "name": "Report_Incardona_Biagio.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/biagio-incardona/biagio-incardona.github.io/blob/master/Report_Incardona_Biagio.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JOdniV63j8gy"
      },
      "source": [
        "# <center>Health Insurance Cross Sell Prediction Report</center>\n",
        "\n",
        "![intro](https://gitlab.com/gerasia/temp/-/raw/master/new_intro.jpg)\n",
        "<div style=\"text-align: right\">  <b>MADE BY</b>: Incardona Biagio</div>\n",
        "<div style=\"text-align: right\">  <b>UNIVERSITY ID (MATRICOLA)</b>: 1000023753 </div>\n",
        "\n",
        "## Context\n",
        "\n",
        "Object to study of this report is the ***Health Insurance Cross Sell Prediction*** dataset, freely available on [Kaggle](https://www.kaggle.com/anmolkumar/health-insurance-cross-sell-prediction?select=train.csv).\n",
        "\n",
        "The dataset consists in a single *csv* file containing informations about some potential customers of an Healt Insurance company. \n",
        "The Insurance company has provided Health Insurance to its customers now they need to identify if there are some   policyholders (customers) that will also be interested in Vehicle Insurance provided by the company.\n",
        "\n",
        "## Content\n",
        "\n",
        "The dataset has approximately 400.000 rows and 12 columns. These columns contains personal informations about the potential customer like age, gender, region code, but also informations about his car (if the user has one) and his life as a driver (if the user can drive). Since the dataset is labeled it contains also a *Response* variable, which tell us if the user has accepted the new contract or not. Sice we won't do a classification algorithm, we can delete this column or use it just for validating the clusters we will find (if any).\n",
        "\n",
        "## Disclaimer\n",
        "\n",
        "Since our machine is really powerless, using the entire dataset for some operations will takes even more than 4 hours of computing (real test, not finished the computation) or even a crash of the system, we will apply a random sampling of the dataset in order to reduce the number of rows from about 400.000 to 5.000 units and we will do all the analysis and discussions on the reduced dataset.\n",
        "\n",
        "I know that in this way we are losing a lot of important informations and there are a lot of probabilities that the row reduction operation will be an high negative bias on the analysis, but this was the only way to compute some operations in a reasonable time! For example it's impossible to compute a $400.000 \\times 400.000$ distance matrix \n",
        "\n",
        "## Packages\n",
        "\n",
        "The following code cell will be used to import all the packages needed to this report"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NwRhZ1C-j8gy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a9d90b2c-bb2b-4a80-85e4-783de8375727"
      },
      "source": [
        "# shut down all the warnings\n",
        "options(warn = -1)\n",
        "install.packages(c(\"devtools\",\"Hmisc\", \"data.table\", \"NbClust\",\"corrplot\",\"spgs\", \"factoextra\", \"ggridges\", \"tidyr\", \"clustertend\", \"vcd\", \"ltm\",\"StatMatch\"))\n",
        "library(ggplot2, warn.conflicts = F, quietly = T)\n",
        "library(dplyr, warn.conflicts = F, quietly = T)\n",
        "library(Hmisc, warn.conflicts = F, quietly = T)\n",
        "library(data.table, warn.conflicts = F, quietly = T)\n",
        "library(corrplot, warn.conflicts = F, quietly = T)\n",
        "library(factoextra, warn.conflicts = F, quietly = T)\n",
        "library(ggridges, warn.conflicts = F, quietly = T)\n",
        "library(tidyr, warn.conflicts = F, quietly = T)\n",
        "library(clustertend, warn.conflicts = F, quietly = T)\n",
        "library(vcd, warn.conflicts = F, quietly = T)\n",
        "library(ltm, warn.conflicts = F, quietly = T)\n",
        "library(StatMatch,warn.conflicts = F, quietly = T)\n",
        "library(cluster)\n",
        "library(devtools)\n",
        "install_github(\"biagio-incardona/myClValid\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Installing packages into ‘/usr/local/lib/R/site-library’\n",
            "(as ‘lib’ is unspecified)\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y6-VIipbj8gy"
      },
      "source": [
        "## 1. Preliminar analysis and data cleaning\n",
        "\n",
        "Let's start importing the file into the system."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mhzQR96Mj8gy"
      },
      "source": [
        "# Loading the dataset into the 'insurance.data' variable\n",
        "insurance.data <- read.csv('Healt_Insurance.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j2DSlgQgj8gz"
      },
      "source": [
        "First of all we will get a random sampled version of the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sQdbSfiGj8gz"
      },
      "source": [
        "set.seed(12999)\n",
        "\n",
        "rows <- sample(1:nrow(insurance.data), 5000)\n",
        "\n",
        "insurance.data <- insurance.data[rows,]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0gfOxdc0j8gz"
      },
      "source": [
        "Let's show the table."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GxTG5Te4j8gz"
      },
      "source": [
        "head(insurance.data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l_f_zHnpj8gz"
      },
      "source": [
        "Now let's see if there are missing values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NFDrrjvDj8gz"
      },
      "source": [
        "# for each value check if is missing or not\n",
        "df.na <- is.na(insurance.data)\n",
        "\n",
        "# for each column check how many missing values are there\n",
        "apply(df.na, 2, sum)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_SurVOxvj8gz"
      },
      "source": [
        "Fortunately the dataset has not any missing values, so we can go on with our analysis without any fear.\n",
        "\n",
        "Initially we will analyze and discuss the dataset structure."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JLeDcaQHj8gz"
      },
      "source": [
        "str(insurance.data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iNQEjJvlj8gz"
      },
      "source": [
        "This dataset contains $n = 5000$ observations and $d = 12$ columns. Let's understand something about the variables:\n",
        "\n",
        "### 1.1 ID"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MKVq41vdj8gz"
      },
      "source": [
        "str(insurance.data$id)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T6lCrjUfj8gz"
      },
      "source": [
        "It contains the identification number of each row and seems to be a 'row number count' variable with no particular informations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KbicXPXJj8gz"
      },
      "source": [
        "# loading the rownames as integer into the 'indexs' variable\n",
        "indexs <- as.integer(rownames(insurance.data))\n",
        "\n",
        "# loading the id column into the 'ids' variable\n",
        "ids <- insurance.data$id\n",
        "\n",
        "# checking if the two vectors are equal\n",
        "min(indexs == ids) == TRUE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RprbmmIIj8gz"
      },
      "source": [
        "Since the 'id' column is exactly a 'row number count' variable whe can delete it without losing informations "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0JxwpBSfj8gz"
      },
      "source": [
        "insurance.data$id <- NULL"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ThFbPQcqj8gz"
      },
      "source": [
        "according to the reducted dimension of the dataset we will set the rownames as the vector  $[1,...,nrow(dataset)]$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ASzatQlFj8gz"
      },
      "source": [
        "rownames(insurance.data) <- 1:nrow(insurance.data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5u8dvZIQj8g0"
      },
      "source": [
        "### 1.2. Gender"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z4J2POtEj8g0"
      },
      "source": [
        "str(insurance.data$Gender)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tn7AI9Iuj8g0"
      },
      "source": [
        "This is a categorical variable, containing informations about the gender of a potential customer.\n",
        "\n",
        "We will transform it in a factor variable with 2 levels: \"Female\" and \"Male\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ObtKTAOKycXb"
      },
      "source": [
        "insurance.data$Gender <- as.factor(insurance.data$Gender)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Okiv1yJej8g0"
      },
      "source": [
        "print(head(insurance.data$Gender))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B2jEUNIij8g0"
      },
      "source": [
        "Let's see how the two genders are distributed into the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bfpQ3OPlj8g0"
      },
      "source": [
        "insurance.data %>%\n",
        "    ggplot(aes(x = Gender, fill = Gender)) +\n",
        "    geom_bar(aes(y = (..count..)/sum(..count..))) +\n",
        "    ylab('frequency')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JG_cEHLEj8g0"
      },
      "source": [
        "As we can see thanks to the bar plot the two genders are pretty equally distributed into the dataset, since the **Females** are approximately the $45\\%$ of the data and the **Males** are approximately the $55\\%$ of the data.\n",
        "\n",
        "### 1.3. Age"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ryG2_gcDj8g0"
      },
      "source": [
        "str(insurance.data$Age)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c1tp-qspj8g0"
      },
      "source": [
        "This is an integer variable containing the potential customer's age.\n",
        "This could be an important information since the higher the age is the more the risks are and the insurance premium could become higher, but the thing is also true in reverse, as younger people are less experienced and generally more reckless.\n",
        "\n",
        "Let's see how this important variable is distributed"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x0d9BQZqj8g0"
      },
      "source": [
        "insurance.data %>%\n",
        "    ggplot(aes(x = Age)) +\n",
        "    geom_bar(aes(y = (..count..)/sum(..count..)), fill = \"blue\") +\n",
        "    ylab(\"frequency\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z4cyne1Rj8g0"
      },
      "source": [
        "It's interesting to see that the main part of the potential customers is between $20$ and $30$ years-old, it could mean that they are looking for their first insurance contract, so, since they are freshman they could be convinced easilier to sign with the company.\n",
        "\n",
        "We can also see another peak in the plot, at the age interval $40-50$. This may be due to people having earned a fair amount of money and being looking for a better insurance contract.\n",
        "\n",
        "### 1.4. Driving_License"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z7cd-U7Sj8g0"
      },
      "source": [
        "str(insurance.data$Driving_License)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6QZEADd6j8g0"
      },
      "source": [
        "This is an integer variable and it may be ambiguous since it could count how many driving licenses one person have (car, truck, motorbike, ...) or it could just be a boolean value indicating if one person has a driving lincense or not.\n",
        "\n",
        "First of all we will see how the data are distributed"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SxaesfzIj8g0"
      },
      "source": [
        "insurance.data %>%\n",
        "    ggplot(aes(x = Driving_License)) +\n",
        "    geom_bar(aes(y = (..count..)/sum(..count..)),, fill = \"blue\") +\n",
        "    ylab(\"frequency\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SXCRnr3Mj8g0"
      },
      "source": [
        "As we can see the variable act like a boolean one. This variable seems to be pretty useless since approximately the $100\\%$ of the people has a driving license.\n",
        "\n",
        "But, the absence of the driving license may significantly change the premium amount and be a crucial factor in the response of the non-driving people.\n",
        "\n",
        "In order to make the dataset consistent we will transform this variable in a categorical one"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oj21gEuGj8g0"
      },
      "source": [
        "insurance.data$Driving_License <- as.factor(if_else(insurance.data$Driving_License == 1, \"Yes\", \"No\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JjbKN9Ihj8g0"
      },
      "source": [
        "### 1.5. Region_Code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VHW1Yvw4j8g0"
      },
      "source": [
        "str(insurance.data$Region_Code)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JCBma6T4j8g0"
      },
      "source": [
        "This integer variable contains an unique code for the region of the customer. Let's see the distribution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n6D1Ht8Oj8g0"
      },
      "source": [
        "insurance.data %>%\n",
        "    ggplot(aes(x = Region_Code)) +\n",
        "    geom_bar(aes(y = (..count..)/sum(..count..)),fill = \"blue\") +\n",
        "    ylab(\"frequency\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TQqlAXF7j8g1"
      },
      "source": [
        "from this graph we can see that most of the customers come from $7$ of the $52$ regions.\n",
        "\n",
        "### 1.6. Previously_Insured"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_qHty6TZj8g1"
      },
      "source": [
        "str(insurance.data$Previously_Insured)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WIf5vkBRj8g1"
      },
      "source": [
        "This is another integer logical like variable that contains information about any previous vehicle insurance. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T0p685llj8g1"
      },
      "source": [
        "insurance.data %>%\n",
        "    ggplot(aes(x = Previously_Insured, fill = Previously_Insured)) +\n",
        "    geom_bar(aes(y = (..count..)/sum(..count..)), fill = \"blue\") +\n",
        "    ylab(\"frequency\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jrBSR4h_j8g1"
      },
      "source": [
        "Also this variable seems to be pretty equally distributed with a small overcoming of people with have no any insurance yet, has to be noted that this could be related to the fact that the most part of the dataset is composed by young people with their (probably) first car!\n",
        "\n",
        "Let's make this variable categorical"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vf1BnAIpj8g1"
      },
      "source": [
        "insurance.data$Previously_Insured <- as.factor(if_else(insurance.data$Previously_Insured == 1, \"Yes\", \"No\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JDyGo866j8g1"
      },
      "source": [
        "### 1.7. Vehicle_Age"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g4OPpjVBj8g1"
      },
      "source": [
        "str(insurance.data$Vehicle_Age)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I_JaRfIWj8g1"
      },
      "source": [
        "This variable is a 3 levels factor, since the car's age is an important factor due to the amount of the annual premium, it could make sense to think that the older it is the higher the prize will be, we need to make sure these factors are ordered"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HANlvo0tj8g1"
      },
      "source": [
        "is.ordered(insurance.data$Vehicle_Age)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VCA3FspCj8g1"
      },
      "source": [
        "The variable is not ordered, so we have to do this by our own"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TRDXZvDFj8g1"
      },
      "source": [
        "levels(insurance.data$Vehicle_Age)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3AVI2w7Zj8g1"
      },
      "source": [
        "# ordering the factors\n",
        "insurance.data$Vehicle_Age <- ordered(insurance.data$Vehicle_Age, levels = c('< 1 Year','1-2 Year', '> 2 Years'))\n",
        "\n",
        "# check if the variable becomes ordered\n",
        "is.ordered(insurance.data$Vehicle_Age)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H9sFKwASj8g1"
      },
      "source": [
        "# check that the order is correct\n",
        "print(head(insurance.data$Vehicle_Age))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xoFXXqPNj8g1"
      },
      "source": [
        "Now we can see how the factors are distributed"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dY6zHLO7j8g1"
      },
      "source": [
        "insurance.data %>%\n",
        "    ggplot(aes(x = Vehicle_Age, fill = Vehicle_Age)) +\n",
        "    geom_bar(aes(y = (..count..)/sum(..count..))) +\n",
        "    ylab(\"frequency\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CPp6nbCYj8g1"
      },
      "source": [
        "The majority of the cars are pretty new, so may have sense to think that this variable has not a lot of utility.\n",
        "\n",
        "Since it is an ordered categorical variable we preffer to transform it in a numeric variable"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AarGfUrej8g1"
      },
      "source": [
        "insurance.data$Vehicle_Age <- as.integer(insurance.data$Vehicle_Age)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "acfa0suaj8g1"
      },
      "source": [
        "### 1.8. Vehicle_Damage\t"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "03JyxFoej8g1"
      },
      "source": [
        "str(insurance.data$Vehicle_Damage)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rH7rKNMSj8g2"
      },
      "source": [
        "This is a char variable acting like a 2-level factorial variable, containing information on whether or not the customer got his/her vehicle damaged. \n",
        "\n",
        "We are going to transform it in a factor variable."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VKETrvWyy336"
      },
      "source": [
        "insurance.data$Vehicle_Damage <- as.factor(insurance.data$Vehicle_Damage)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FYWJrQ05j8g2"
      },
      "source": [
        "\n",
        "Let's see the distribution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y6v1qpNhj8g2"
      },
      "source": [
        "insurance.data %>%\n",
        "    ggplot(aes(x = Vehicle_Damage, fill = Vehicle_Damage)) +\n",
        "    geom_bar(aes(y = (..count..)/sum(..count..))) +\n",
        "    ylab(\"frequency\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qwsIYLmNj8g2"
      },
      "source": [
        "The variable is \n",
        "almost perfectly balanced, so no comments can be made\n",
        "\n",
        "### 1.9. Annual_Premium"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vuRgXoKQj8g2"
      },
      "source": [
        "str(insurance.data$Annual_Premium)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "giZLkZcIj8g2"
      },
      "source": [
        "Numerical variable indicating the amount customer needs to pay as premium in the year"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "22k7CSkAj8g2"
      },
      "source": [
        "# get some break points for a better visualization of the x-axis\n",
        "breaks.x <- c(\n",
        "    min(insurance.data$Annual_Premium),\n",
        "    max(insurance.data$Annual_Premium)/4,\n",
        "    max(insurance.data$Annual_Premium)/2,\n",
        "    max(insurance.data$Annual_Premium)/1.35,\n",
        "    max(insurance.data$Annual_Premium))\n",
        "    \n",
        "insurance.data %>%\n",
        "    ggplot(aes(x = Annual_Premium)) +\n",
        "    geom_histogram(aes(y = ..density..),bins = 45, fill = \"blue\") +\n",
        "    scale_x_continuous(breaks = breaks.x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OrHjiqsIj8g2"
      },
      "source": [
        "We can see that the most premium is in the interval $(2630,115000)$ with a really low number of people which falls out of that interval."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MdaFbwYqj8g2"
      },
      "source": [
        "### 1.10. Policy_Sales_Channel"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "24oyCbdtj8g2"
      },
      "source": [
        "str(insurance.data$Policy_Sales_Channel)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fww3h7Dcj8g2"
      },
      "source": [
        "This variable contains anonymized Code for the channel of outreaching to the customer ie. Different Agents, Over Mail, Over Phone, In Person, etc."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R33tUF7cj8g2"
      },
      "source": [
        "insurance.data %>%\n",
        "    ggplot(aes(x = Policy_Sales_Channel)) +\n",
        "    geom_bar(aes(y = (..count..)/sum(..count..)),fill = \"blue\") +\n",
        "    ylab(\"frequency\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JbqDyqu7j8g2"
      },
      "source": [
        "There are few main policy sales channels, but due to the fact that there are no informations about what a channel refers to, we can't do any discussion."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mddF9Ufrj8g2"
      },
      "source": [
        "### 1.11. Vintage"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sY3UrCWKj8g2"
      },
      "source": [
        "str(insurance.data$Vintage)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YicIvlbRj8g2"
      },
      "source": [
        "This variable contains the number of days a customer has been associated with the company. The fidelity of a person could be a nice bias in order to sign the contract. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "llIG1IwOj8g2"
      },
      "source": [
        "Let's see the distribution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WP3PFVydj8g2"
      },
      "source": [
        "insurance.data %>%\n",
        "    ggplot(aes(x = Vintage)) +\n",
        "    geom_bar(aes(y = ..count..),fill = \"red\") +\n",
        "    geom_density(aes(y = ..count..),color = \"blue\") +\n",
        "    ylab(\"frequency\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qZBDPPpsj8g2"
      },
      "source": [
        "The variable seems to be uniformly distributed.\n",
        "\n",
        "Let's test this whit a significance level $\\alpha = 0.05$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QEAnRMWCj8g2"
      },
      "source": [
        "install.packages(\"spgs\")\n",
        "scaled.vintage <- scale(insurance.data$Vintage, scale = T, center = F)\n",
        "min <- min(scaled.vintage)\n",
        "max <- max(scaled.vintage)\n",
        "test <- spgs::chisq.unif.test(scaled.vintage, interval = c(min,max), bins = nrow(insurance.data))\n",
        "test$p.value"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WUBDZzupj8g2"
      },
      "source": [
        "Since whe have that $p-value > \\alpha = 0.05$ the test told us that we cannot reject the hypotesis that the variable is uniformly distributed!\n",
        "\n",
        "We have that the dataset is equally distributed between fresh customers and older ones. But also the older ones are customers for less than one year!\n",
        "\n",
        "### 1.12. Response\n",
        "\n",
        "This is the label column, since we want do some unsupervised analysis we will delete without even looking at it since it could be an high bias on our analysis and choices."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XBQZ01Cej8g2"
      },
      "source": [
        "insurance.data$Response <- NULL"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PBbQlKdbj8g2"
      },
      "source": [
        "<hr style=\"height:2px;border-width:0;color:black;background-color:black\">\n",
        "\n",
        "## <center> Variables Recap</center>\n",
        "\n",
        "|#| Name | Type | Description |\n",
        "|:-| :- | -: | :-: |\n",
        "|1| [Gender](#1.2.-Gender) | Factor | Gender of the customer|\n",
        "|2| [Age](#1.3.-Age) | Integer | Age of the customer |\n",
        "|3| [Driving_License](#1.4.-Driving_License) | Factor | &emsp;&emsp;&emsp; <span style=\"color:green\">**Yes**</span>: Customer already has DL; <span style=\"color:red\">**No**</span>: Customer does not have DL| \n",
        "|4| [Region_Code](#1.5.-Region_Code) | Integer | Unique code for the region of the customer |\n",
        "|5| [Previously_Insured](#1.6.-Previously_Insured) | Factor | &emsp;&emsp;&emsp; <span style=\"color:green\">**Yes**</span>: Customer already has Vehicle Insurance;<span style=\"color:red\">**No**</span>: Customer doesn't have Vehicle Insurance|\n",
        "|6|[Vehicle_Age](#1.7.-Vehicle_Age)|Integer| Age of the Vehicle|\n",
        "|7|[Vehicle_Damage](#1.8.-Vehicle_Damage)|Factor|&emsp;&emsp;&emsp; <span style=\"color:green\">**Yes**</span>:Customer got his/her vehicle damaged in the past;<span style=\"color:red\">**No**</span>:Customer didn't get his/her vehicle damaged in the past|\n",
        "|8|[Annual_Premium](#1.9.-Annual_Premium)|Numeric|The amount customer needs to pay as premium in the year|\n",
        "|9|[Policy_Sales_Channel](#1.10.-Policy_Sales_Channel)|Integer|Anonymized Code for the channel of outreaching to the customer ie. Different Agents, Over Mail, Over Phone, In Person, etc.|\n",
        "|10| [Vintage](#1.11.-Vintage)|Integer|Number of Days the Customer has been associated with the company|\n",
        "\n",
        "<hr style=\"height:2px;border-width:0;color:black;background-color:black\">\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iyn2CyEIj8g3"
      },
      "source": [
        "## 2. Preparing the data\n",
        "\n",
        "Before going on with our analysis we need to apply some transformations to the dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iGzK1YrFj8g3"
      },
      "source": [
        "\n",
        "### 2.1 Data Normalization\n",
        "\n",
        "We have really different data ranges in our dataset, so we have to scale the data in order to make the future analysis indipendent from the scale of the data.\n",
        "\n",
        "**NOTE**: this does not affect the data distributions seen above and their respective discussions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RTgxu23qj8g3"
      },
      "source": [
        "# get the numeric variables\n",
        "num_cols <- unlist(lapply(insurance.data, is.numeric))  \n",
        "\n",
        "# scale the numeric variables\n",
        "insurance.scaled <- insurance.data\n",
        "insurance.scaled[,num_cols] <- as.data.frame(scale(insurance.data[num_cols],center = F, scale = T))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L8XVode-j8g3"
      },
      "source": [
        "Let's show the new dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "udmKd5Ybj8g3"
      },
      "source": [
        "head(insurance.scaled)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WTT_rYnRj8g3"
      },
      "source": [
        "## 3. Correlation analysis\n",
        "\n",
        "It's important to see if there is any kind of relations between variables.\n",
        "\n",
        "### 3.1 Pearson's correlation\n",
        "\n",
        "Let's start checking if there is correlation between numerical values.\n",
        "\n",
        "First of all we will define a function to create a matrix of p-value according to the Pearons's linearity test, after we will calculate the Pearson's correlation matrix and the p-values matrix to check if there is a linear depencies between the variables. We will use a significativity level of $0.01$ to test our linearity correlation hipothesis. At the end we will show the results via an heatmap. In the next figure, correlations with p-value > 0.01 are considered as insignificant. In this case the correlation coefficient values are leaved blank and crosses are added."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KzjGtDg-j8g3"
      },
      "source": [
        "# a function to create a matrix filled by p-value of the correlation tests\n",
        "cor.mtest <- function(mat, ...) {\n",
        "    mat <- as.matrix(mat)\n",
        "    n <- ncol(mat)\n",
        "    p.mat <- matrix(NA, n, n)\n",
        "    for (i in 1:n) {\n",
        "        for (j in 1:n) {\n",
        "            tmp <- cor.test(mat[, i], mat[, j], ...)\n",
        "            p.mat[i, j] <- tmp$p.value\n",
        "        }\n",
        "    }\n",
        "  colnames(p.mat) <- rownames(p.mat) <- colnames(mat)\n",
        "  return(p.mat)\n",
        "}\n",
        "\n",
        "# calculate the correlation matrix\n",
        "cor <- cor(insurance.scaled[,num_cols], method = \"pearson\")\n",
        "\n",
        "# calculate the correlation p-value matrix\n",
        "p.mat.pears <- cor.mtest(insurance.scaled[,num_cols])\n",
        "\n",
        "# showing the correlation matrix with an heatmap\n",
        "corrplot(cor, type = \"lower\", p.mat = p.mat.pears, sig.level = 0.01, method = \"color\",addCoef.col = \"black\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KpY51Snwj8g3"
      },
      "source": [
        "One thing that we can see in the plot is that the vehicle age is positively correlated to the customers' age.\n",
        "\n",
        "There is also a negative correlation between **policy_sales_channel** and **Age**, but since this is a non ordered categorical variable it makes no sense to consider that information, however if we had known that the policy_sales_channel has been chronologically ordered (the $i^{th}$ sales channel was added after the $(i-1)^{th}$ sales channel and before the $(i+1)^{th}$ sales channel) we had that newer (bigger value) sales channels work better with younger customers and older (lower value) sales channels work better with older customers!\n",
        "\n",
        "Overall we can see that there are only few variables that are strongly linearly correlated.\n",
        "\n",
        "Note that the Pearson correlation matrix shows only if two variables are **linearly** correlated, so makes sense study also other relationships based on our intuition in order to see if there are any non-linear correlation between data!\n",
        "\n",
        "### 3.2 Cramer's V for correlation\n",
        "\n",
        "Now let's apply the Chi-Square test to test if there is any form of correlation between categorical variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LiKUPGkGj8g3"
      },
      "source": [
        "# a function to create a matrix filled by cramers value for the correlation\n",
        "cram.cor <- function(mat, ...) {\n",
        "    mat <- as.matrix(mat)\n",
        "    n <- ncol(mat)\n",
        "    p.mat <- matrix(NA, n, n)\n",
        "    for (i in 1:n) {\n",
        "        for (j in 1:n) {\n",
        "            tmp <- assocstats(table(mat[,i],mat[,j]))\n",
        "            p.mat[i, j] <- tmp$cramer\n",
        "        }\n",
        "    }\n",
        "  colnames(p.mat) <- rownames(p.mat) <- colnames(mat)\n",
        "  return(p.mat)\n",
        "}\n",
        "\n",
        "cram.matrix <- cram.cor(insurance.scaled[,!num_cols])\n",
        "\n",
        "corrplot(cram.matrix, type = \"lower\", method = \"color\", is.corr = F, addCoef.col = \"black\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OSGAaMTjj8g3"
      },
      "source": [
        "Here we have plotted the cramers' values.\n",
        "\n",
        "We can see that *gender* is weakly related with *Previously_Insured* and *Vehicle_Damage*, instead *Vehicle_Damage* is highly related to *Previously_Insured*!\n",
        "\n",
        "### 3.3 Biserial correlation\n",
        "\n",
        "Finally we will analyze if there is any form of correlation between mixed (numeric vs categorical) variables, we are going to to this by applying the Biserial correlation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9bq6PzGtj8g3"
      },
      "source": [
        "biserial.cormat <- function(mat1, mat2, ...) {\n",
        "    n1 <- ncol(mat1)\n",
        "    n2 <- ncol(mat2)\n",
        "    p.mat <- matrix(NA, n1, n2)\n",
        "    for (i in 1:n1) {\n",
        "        for (j in 1:n2) {\n",
        "            if(is.numeric(mat1[,i]) == T && is.factor(mat2[,j]) == T){\n",
        "                tmp <- biserial.cor(x = mat1[,i], y = mat2[,j], use = \"all.obs\")\n",
        "                p.mat[i, j] <- tmp\n",
        "            }\n",
        "\n",
        "        }\n",
        "    }\n",
        "  colnames(p.mat) <- colnames(mat2)\n",
        "  rownames(p.mat) <- colnames(mat1)\n",
        "  return(t(p.mat))\n",
        "}\n",
        "\n",
        "biserial.matrix <- biserial.cormat(insurance.scaled[,num_cols], insurance.scaled[,!num_cols])\n",
        "corrplot(biserial.matrix,method = \"color\", is.corr = F, addCoef.col = \"black\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_a6sneBtj8g3"
      },
      "source": [
        "Thanks to the Biserial correlation we can see that there is correlation between numerical and categorical variables, except *Region_Code*,*Annual_Premium* and *Vintage* that seem to are indipendent from all the categorical variables."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lx04zxOCj8g3"
      },
      "source": [
        "## 4. Multivariate analysis\n",
        "\n",
        "We will analyze in depth the relationships between some interesting variables in order to have a better understanding about the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KklusLDBj8g3"
      },
      "source": [
        "### 4.1. Age~Vehicle_Age\n",
        "\n",
        "Let's see a box plot that reppresent these two variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "03K35ljij8g3"
      },
      "source": [
        "insurance.data %>%\n",
        "    ggplot(aes(x = Vehicle_Age, y = Age)) +\n",
        "    geom_boxplot(aes(group=Vehicle_Age, colour = as.factor(Vehicle_Age))) +\n",
        "    guides(color=guide_legend(title=\"Vehicle_Age\")) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OABd6I9Fj8g3"
      },
      "source": [
        "Looking at the plot we can see that the newest cars ($< 1$ year) are driven by people between $20$ and $30$ years, the mid-age cars (between $1$ and $2$ years) are driven by mid-age people ($40-60$ years old) and the older cars are driven also by the mid-age people but few years older. According to the Vehicle Age [plot](#1.7.-Vehicle_Age) we know there are a really low number of old cars, so we have that the customers, in general, all have discrete security systems in their cars!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1oJ0P4eOj8g3"
      },
      "source": [
        "### 4.2. Age~Vehicle_Damage\n",
        "\n",
        "It's important to see if people have their vehicles damaged with a certain relation to their ages."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6F8jtlrTj8g3"
      },
      "source": [
        "insurance.data %>%\n",
        "    ggplot(aes(x = Age)) +\n",
        "    geom_histogram(aes(y = ..count../sum(..count..),fill = Vehicle_Damage), bins = 40)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t582jOxJj8g3"
      },
      "source": [
        "Thanks to the plot we know that in general people tend to not have their vehicle damaged. We have also that youger people are more cautious than older ones. We can also see that the differences between red (no damages) and blue (damaged cars) bars become lower and lower at the increasing of the age."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lEo59hpbj8g3"
      },
      "source": [
        "### 4.3. Vehicle_Age~Vehicle_Damage\n",
        "\n",
        "We already know that there is a certain form of relation between these data, this could be due the more a car is driven the more are the probability to get damaged."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QW5rDnloj8g3"
      },
      "source": [
        "insurance.data %>%\n",
        "    ggplot(aes(x = as.factor(Vehicle_Age))) +\n",
        "    geom_bar(aes(y =..count../sum(..count..), fill = Vehicle_Damage), position = \"dodge\") +\n",
        "    xlab(\"Vehicle_Age\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Szcpnin9j8g3"
      },
      "source": [
        "The plot seems to accord our expectations, since we can see that the proportion of damaged cars increase at the increasing of the age of the cars"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RMnC0uqoj8g3"
      },
      "source": [
        "### 4.4. Age~Annual_Premium\n",
        "\n",
        "According to the two discussions above, it is reasonable to think that older people, having on average older cars, are more at risk of accidents than younger people. Hence, let's see if the age is a relevant factor on the calculation of the annual premium"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-fXUn8i9j8g3"
      },
      "source": [
        "insurance.data %>%\n",
        "    ggplot(aes(x = Age, y = Annual_Premium)) + \n",
        "    geom_point() +\n",
        "    geom_smooth(formula =  y ~ x, method = \"loess\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CUuvMRGZj8g3"
      },
      "source": [
        "No relations seems to be between these two features, since the annual premium is pretty equal distributed between ages. Since there are a lot of data could be difficult to see some relations, let's try to plot a random k-sample of the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3OJC0REwj8g3"
      },
      "source": [
        "# define k, the dimension of the sample\n",
        "k = 1000\n",
        "\n",
        "# get the k-dimensional sample\n",
        "sample1 <- sample(1:nrow(insurance.scaled), k)\n",
        "sample2 <- sample(1:nrow(insurance.scaled), k)\n",
        "sample3 <- sample(1:nrow(insurance.scaled), k)\n",
        "\n",
        "#plot the samples\n",
        "insurance.data[sample1, ] %>%\n",
        "    ggplot(aes(x = Age, y = Annual_Premium)) + \n",
        "    geom_point() +\n",
        "    geom_smooth(formula =  y ~ x, method = \"loess\")\n",
        "\n",
        "insurance.data[sample2, ] %>%\n",
        "    ggplot(aes(x = Age, y = Annual_Premium)) + \n",
        "    geom_point() +\n",
        "    geom_smooth(formula =  y ~ x, method = \"loess\")\n",
        "\n",
        "insurance.data[sample3, ] %>%\n",
        "    ggplot(aes(x = Age, y = Annual_Premium)) + \n",
        "    geom_point() +\n",
        "    geom_smooth(formula =  y ~ x, method = \"loess\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "34AjBudHj8g3"
      },
      "source": [
        "After we have plotted different samples nothing seems to appear, so we can think that the customer's age is not a determinant variable to the annual prize!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CHyTY-UIj8g3"
      },
      "source": [
        "### 4.5. Vehicle_Age~Annual_Premium\n",
        "\n",
        "Let's see how the vehicle age influence the annual premium. Since the Annual_Premium variable is a continuous variable with a really large range, we will delete the last part due to the fact that there are really low samples with an annual premium greather than $100,000\\$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gt2NSjb9j8g3"
      },
      "source": [
        "insurance.data %>%\n",
        "    ggplot(aes(x = Annual_Premium,fill = as.factor(Vehicle_Age), color = as.factor(Vehicle_Age))) +\n",
        "    geom_density(alpha = 0.3, size = 1) +\n",
        "    coord_cartesian(xlim = c(-1000,90000)) +\n",
        "    guides(color=guide_legend(title=\"Vehicle_Age\"), fill=guide_legend(title=\"Vehicle_Age\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IuF6NnIXj8g4"
      },
      "source": [
        "We can see that there are a lot of sample with the lower annual price, indipendently from the vehicle age, but leaving these \"marginal\" points we can see a slighty difference between the premium according to the vehicle ages, the premium increase at the incrasing of the age."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FrN8Fc_Zj8g4"
      },
      "source": [
        "### 4.6. Vehicle_Damage~Annual_Premium\n",
        "\n",
        "We expect that if a vehicle got damaged the annual premium increase, but let's see if this really happens"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1qCjUJFqj8g4"
      },
      "source": [
        "insurance.data %>%\n",
        "    ggplot(aes(x = Annual_Premium,color = Vehicle_Damage, fill = Vehicle_Damage)) +\n",
        "    geom_density(alpha = 0.3, size = 1) +\n",
        "    coord_cartesian(xlim = c(6000,100000))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "34qYKJvrj8g4"
      },
      "source": [
        "As we can see, the damage have not a significat weight on the calculation of the annual premium, if a car is damaged there are only few more probability to pay an higher premium. We can say that by looking at the small right shifting of the Damaged Vehicle distribution but in average people pay the same amount for the annual premium indipendently if the vehicle is damaged or not."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BmzRRekQj8g4"
      },
      "source": [
        "### 4.7. Vehicle_Age~Vehicle_Damage-Annual_Premium\n",
        "\n",
        "Since both Vehicle_Age and Vehicle_Damage variables seems to give a limited contribute to the annual prize, let's see how this two variables contribute together"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qvk7540Hj8g4"
      },
      "source": [
        "insurance.data %>%\n",
        "    ggplot(aes(x = Annual_Premium,color = as.factor(Vehicle_Age), fill = as.factor(Vehicle_Age))) +\n",
        "    geom_density(alpha = 0.3, size = 1) +\n",
        "    coord_cartesian(xlim = c(6000,100000)) +\n",
        "    facet_wrap(~Vehicle_Damage)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "acHdY8wDj8g4"
      },
      "source": [
        "Thanks to this plot we can see an interesting fact:\n",
        "\n",
        "- There are no old (>2 years; blue bell) cars without damages;\n",
        "\n",
        "- A person with an older car and with no damage is awarded in the annual premium rather than the youger car, instead if a person with an old car got his car damaged, then the annual premium become, in general, higher than the newest cars!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t4wCGnkoj8g4"
      },
      "source": [
        "## 5. Principal Components Analysis\n",
        "\n",
        "It is important to understand the variablity of the data, to do so we will apply the Principal Component Analysis in order to check out which variables explains more variablity.\n",
        "\n",
        "We are going to apply the PCA only on the numeric variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0l4DO9cXj8g4"
      },
      "source": [
        "# calculate pca\n",
        "pca <- prcomp(insurance.scaled[,num_cols], scale. = F, center = F)\n",
        "\n",
        "# show some informations\n",
        "(summary.pca <- summary(pca))\n",
        "\n",
        "pca"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xSo2ZJllj8g4"
      },
      "source": [
        "Thanks to this summary we can see that with the first 2 cover aproximately $90\\%$ of the variability!\n",
        "\n",
        "But let's analyze it graphically"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3OwdwyYmj8g4"
      },
      "source": [
        "plot(summary.pca$importance[3,], xlab = \"Principal Component\",\n",
        "     ylab = \"Cumulative Proportion of Variance Explained\",\n",
        "     type = \"b\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qShQyRhEj8g4"
      },
      "source": [
        "We will take only the first $2$ Principal components and leave all the others.\n",
        "\n",
        "Let's plot the dataset in this new set of variables."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wp-yJif2j8g4"
      },
      "source": [
        "# get the first 2 Principal components\n",
        "pc.data <- as.data.frame(pca$x[,1:2])\n",
        "\n",
        "# plot the data\n",
        "plot(pc.data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0fzfzOtjj8g4"
      },
      "source": [
        "Thanks to this plot we can see that, in the new space, there are at least two separate group of customers. The first group could be the one with $PC2 <= -0.5$ and the other points going into the second one\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pfitq8V4j8g4"
      },
      "source": [
        "We could also think to separate the data into 3 groups only based on the second Principal Component.\n",
        "Let's check out the contribute of each variable to each Principal Component."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lOBBijeaj8g4"
      },
      "source": [
        "var <- get_pca_var(pca)\n",
        "\n",
        "corrplot(t(var$contrib[,1:2]), is.corr=FALSE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OFnu8mvRj8g4"
      },
      "source": [
        "As we can see the second Principal Component is mainly determined by *Age*, *Vehicle_Age* and *Policy_Sales_Channel* and we know that this variables are correlated each other. Instead the first principal component is pretty equally composed by all the variables.\n",
        "\n",
        "Thanks to the plot above and the heatmap we have just discussed, we can see that the second Principal Component is pretty well clustered, but looking at the first principal component, which is composed by all the variables, we can see that the most part of the samples form some *clouds of points* but there are also some samples standing outside these clouds.\n",
        "\n",
        "Let's analyze the two Principal Components separately"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KUIAYle0j8g4"
      },
      "source": [
        "pc.data %>% \n",
        "    ggplot(aes(PC1)) + \n",
        "    geom_density(color = \"darkblue\", fill = \"lightblue\") +\n",
        "    geom_vline(aes(xintercept=mean(PC1)), color = \"red\", linetype = \"dashed\", size = 1) +\n",
        "    ggtitle(\"Plot PC1\") +\n",
        "    theme(plot.title = element_text(size = 30))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XVStDrcDj8g4"
      },
      "source": [
        "As we saw in the scatterplot the first Principal Component follow a normal distribution with a little right skewness.\n",
        "\n",
        "Now it's time to look at the second Principal Component."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "38OAH29wj8g4"
      },
      "source": [
        "pc.data %>%\n",
        "    ggplot(aes(PC2)) +\n",
        "    geom_density(color = \"darkblue\", fill = \"lightblue\") +\n",
        "    geom_vline(aes(xintercept=mean(PC2)), color = \"red\", linetype = \"dashed\", size = 1) +\n",
        "    ggtitle(\"Plot PC2\") +\n",
        "    theme(plot.title = element_text(size = 30))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NAhHYqvaj8g4"
      },
      "source": [
        "Here we can see three different peaks, these are the three potential clusters we said before.\n",
        "\n",
        "## 6. Outlier Detection\n",
        "\n",
        "It could be interesting to study the first principal component in order to check out if these outliers are there due to noise on the data or these are some interesting points.\n",
        "\n",
        "### 6.1 Hampel filter\n",
        "\n",
        "Here we will apply a smoother version of the Hampel filter to the first Principal Component hoping that this technique can identify the right samples.\n",
        "\n",
        "This is an outlier detection technique, it consists of considering as outliers the values outside the interval ($I$) formed by the median, plus or minus 3 times the median absolute deviations($MAD$): $$I = [median - 2.5\\cdot MAD; median-2.5\\cdot MAD]$$\n",
        "\n",
        "where MAD is the median absolute deviation and is defined as the median of the absolute deviations from the data’s median $\\tilde{X}=median(X)$ \n",
        "\n",
        "$$MAD = median(|X_i - \\tilde{X}|)$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f9QJ7nN0j8g4"
      },
      "source": [
        "# calculate the lower bound\n",
        "lower_bound <- median(pc.data$PC1) - 2.5 * mad(pc.data$PC1)\n",
        "\n",
        "# calculate the upper bound\n",
        "upper_bound <- median(pc.data$PC1) + 2.5 * mad(pc.data$PC1)\n",
        "\n",
        "# get the outliers indexes\n",
        "left_outlier_ind <- which(pca$x[,1] < lower_bound)\n",
        "right_outlier_ind <- which(pca$x[,1] > upper_bound)\n",
        "\n",
        "# creating a copy of the data\n",
        "hampel.data <- pc.data\n",
        "\n",
        "# set the sampler as outlier\n",
        "hampel.data$is.outlier <- 1:nrow(hampel.data)\n",
        "\n",
        "hampel.data$is.outlier[-left_outlier_ind] = 1 #no outliers\n",
        "hampel.data$is.outlier[left_outlier_ind] = 2 # < lower_bound\n",
        "hampel.data$is.outlier[right_outlier_ind] = 3 # > upper_bound\n",
        "\n",
        "hampel.data$is.outlier <- as.factor(hampel.data$is.outlier)\n",
        "\n",
        "# plot the new data\n",
        "colVar <- sapply(hampel.data$is.outlier, function(a){ifelse(a == 1,'red', ifelse(a == 2,'green', 'blue'))})\n",
        "colVar <- factor(colVar,levels=c('red','green','blue'))\n",
        "\n",
        "hampel.data %>% ggplot(aes(PC1,PC2, color = as.factor(is.outlier))) + geom_point()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s26zo6htj8g4"
      },
      "source": [
        "We caught most of the left side and right side outliers! Let's analyze both the groups."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DhSwRDl6j8g4"
      },
      "source": [
        "temp.data <- as.data.frame(insurance.scaled)\n",
        "temp.data$class <- hampel.data$is.outlier\n",
        "\n",
        "temp.data %>%\n",
        "    gather(variable, value, -class) %>%\n",
        "    ggplot(aes(y = as.factor(variable),\n",
        "              fill = class,\n",
        "              color = class,\n",
        "              x = percent_rank(value))) +\n",
        "    geom_density_ridges(alpha = 0.5) +\n",
        "    xlab(\"percent_rank\") +\n",
        "    ylab(\"variable\") +\n",
        "    scale_fill_discrete(name = \"Dose\", labels = c(\"No Outlier\", \"Left Outlier\", \"Right Outlier\")) +\n",
        "    scale_color_discrete(name = \"Dose\", labels = c(\"No Outlier\", \"Left Outlier\", \"Right Outlier\")) +\n",
        "    guides(fill=guide_legend(title=\"Class\"), color=guide_legend(title=\"Class\")) +\n",
        "    ggtitle(\"Plot\") +\n",
        "    theme(plot.title = element_text(size = 30))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EvwgBVh6j8g5"
      },
      "source": [
        "We can see that *Gender* and *Driving_License* are pretty equal in the three classes. We also have that the left outliers are, in general, old people who have been customers of the company for long time driving old cars and got offered a very high annual premium. The right outliers are exactly the opposite of the left outliers!\n",
        "\n",
        "The *Policy_Sales_Channel* and *Region_Code* seems to be two others crucial variables to the outliers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8U5mnTw1j8g5"
      },
      "source": [
        "## 7. Cluster Analysis\n",
        "\n",
        "In this chapter we will carry out some significant clusters on the dataset (if any)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "crtyyr1dj8g5"
      },
      "source": [
        "### 7.1 Clustering tendency\n",
        "\n",
        "Before going on the cluster analysis we have to assert that the dataset contains meaningful clusters or not. To do that we will use the hopkins statistic and the VAT Algorithm for a graphical representation.\n",
        "\n",
        "#### 7.1.1 VAT Visual Assessment of cluster Tendency\n",
        "\n",
        "First of all we will analyze the VAT results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jdcf7mP7j8g5"
      },
      "source": [
        "# calculate the gower distance matrix\n",
        "dist <- as.dist(gower.dist(insurance.scaled))\n",
        "\n",
        "# VAT\n",
        "#fviz_dist(dist, show_labels = F, gradient = list(low = \"black\", mid = \"white\", high = \"white\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-4uNfT1Aj8g5"
      },
      "source": [
        "The VAT algorithm shows up that the data seems pretty well clustered since there are some *large squares* into the first diagonal of the VAT plot.\n",
        "\n",
        "#### 7.1.2 Hopkins Statistic\n",
        "\n",
        "Even if the VAT told us that the dataset was clustered we will apply a statistical test, the **Hopkins Statistic** to have a numerical value of the cluster tendency of the dataset.\n",
        "The Hopkins Statistic is a statistical way to measure the cluster tendency of a dataset, it will take values in $[0,1]$. Since the dataset is composed by categorical and numeric variables we have to use the gower distance, hence we cannot apply the pre-built R function for the Hopkins statistic but we have to create it by scratch. \n",
        " \n",
        " First of all we will define all the functions which allow us to compute the Hopkins Statistic."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l8tZMx3Tvawc"
      },
      "source": [
        "# function to create the sampled dataset\n",
        "create.sampled.data <- function(m, X){\n",
        "  Y.indexs <- sample(1:nrow(X), m)\n",
        "  Y <- X[Y.indexs,]\n",
        "}\n",
        "\n",
        "# function to create the random artificial dataset\n",
        "create.artificial.data <- function(m, X){\n",
        "  Gender <- as.factor(sample(c(\"Male\", \"Female\"), m, replace = T, prob = c(0.5,0.5)))\n",
        "  Age <- runif(m, min = min(X$Age), max = max(X$Age))\n",
        "  Driving_License <- as.factor(sample(c(\"Yes\", \"No\"), m, replace = T, prob = c(0.5,0.5)))\n",
        "  Region_Code <- runif(m, min = min(X$Region_Code), max = max(X$Region_Code))\n",
        "  Previously_Insured <- as.factor(sample(c(\"Yes\", \"No\"), m, replace = T, prob = c(0.5,0.5)))\n",
        "  Vehicle_Age <- runif(m, min = min(X$Vehicle_Age), max = max(X$Vehicle_Age))\n",
        "  Vehicle_Damage <- as.factor(sample(c(\"Yes\", \"No\"), m, replace = T, prob = c(0.5,0.5)))\n",
        "  Annual_Premium <- runif(m, min = min(X$Annual_Premium), max = max(X$Annual_Premium))\n",
        "  Policy_Sales_Channel <- runif(m, min = min(X$Policy_Sales_Channel), max = max(X$Policy_Sales_Channel))\n",
        "  Vintage <- runif(m, min = min(X$Vintage), max = max(X$Vintage))\n",
        "\n",
        "  # put all together in a dataset\n",
        "  Z <- data.frame(Gender, Age, Driving_License, Region_Code, Previously_Insured, Vehicle_Age, Vehicle_Damage, Annual_Premium, Policy_Sales_Channel, Vintage)\n",
        "  return(Z)\n",
        "}\n",
        "\n",
        "# function to calculate distances\n",
        "calc.dist <- function(X, Y, m, is.original = T){\n",
        "  YX.dists <- 0\n",
        "  if(is.original == T){\n",
        "    for(i in 1:m){\n",
        "      # X[rownames(X) != rownames(Y[i,])[1],] allows us to avoid the distance\n",
        "      #                                       between a row with itself\n",
        "      YX.dists <- YX.dists + min(gower.dist(Y[i,], X[rownames(X) != rownames(Y[i,])[1],]))\n",
        "    }\n",
        "  }\n",
        "  else {\n",
        "    for(i in 1:m){\n",
        "      YX.dists <- YX.dists + min(gower.dist(Y[i,], X))\n",
        "    }\n",
        "  }\n",
        "  return(YX.dists)\n",
        "}\n",
        "\n",
        "# function to calculate hopkins statistic\n",
        "my.hopkins <- function(X, m){\n",
        "  Y <- create.sampled.data(m, X)\n",
        "  Z <- create.artificial.data(m, X)\n",
        "  YX.dist <- calc.dist(X, Y, m, T)\n",
        "  ZX.dist <- calc.dist(X, Z, m, F)\n",
        "  H <- YX.dist / (ZX.dist + YX.dist)\n",
        "  \n",
        "  return(H) \n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FRMgfwl7AlYi"
      },
      "source": [
        "Now we will apply the Hopkins statistic to our dataset with $m = 4000$ (we will generate the two datasets with $m$ rows)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X4pRqyJiyy42"
      },
      "source": [
        "m <- 4000\n",
        "#my.hopkins(insurance.scaled, m)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hQoO9P9pVnjS"
      },
      "source": [
        "We got a really nice value for the test, $0.1$, it means that the dataset is well clusterable and our discussions about the VAT algorithm were right! "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jg5P9BPTj8g5"
      },
      "source": [
        "### 7.2 Optimal number of clusters & Best clustering method\n",
        "\n",
        "Now that we know that the dataset is clusterable we have to check out the best number of clusters.\n",
        "We will compute some different clustering algorithm and evaluatig some plots and statistics in order to check out the best combination: **Number of clusters ~ Algorithm**.\n",
        "\n",
        "We will analyze the following Clustering Algorithms:\n",
        "  * PAM\n",
        "  * Hierarchical (with different configurations)\n",
        "  * DBScan\n",
        "\n",
        "And we will use the following metrics:\n",
        "  * Elbow method\n",
        "  * Average Silhouette method\n",
        "  * Dunn index\n",
        "  * GAP statistic\n",
        "\n",
        "#### 7.2.1 PAM Partitioning Around Medoid\n",
        "\n",
        "The PAM algorithm is generalization of k-means using any distance measure and using elements of the data as centroid (in PAM these are called medoids).\n",
        "\n",
        "Since PAM doesn't use the **WSS** metric we have to build the WSS by our own.\n",
        "\n",
        "In order to use the GAP statistic we have to build our function, because the pre-built function doesn't allow categorical data.\n",
        "\n",
        "Let's write the functions to compute *WSS*, *Elbow Method*,  and *GAP Statistic* respectively.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y8zC5X_1ojwp"
      },
      "source": [
        "#### 7.2.1 Optimal number of clusters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sglZgQWVj8g5"
      },
      "source": [
        "calc.wss <- function(clustering){\n",
        "  WSS <- t(clustering$clusinfo[,3]) %*%  clustering$clusinfo[,1]\n",
        "  return(WSS)\n",
        "}\n",
        "# elbow function\n",
        "pam.elbow <- function(dist, K){\n",
        "  elbow <- c()\n",
        "  for (i in 1:K){\n",
        "    pam_fit <- pam(dist, diss = T, k = i)\n",
        "    elbow[i] <- calc.wss(pam_fit)\n",
        "  }\n",
        "  plot(1:K, elbow, \n",
        "    xlab = \"Number of clusters\",\n",
        "    ylab = \"WSS\",\n",
        "    main = \"WSS Plot\")\n",
        "  lines(1:K, elbow)\n",
        "}\n",
        "\n",
        "step1 <- function(dist, K){\n",
        "  # step 1\n",
        "  obj.fun <- c()\n",
        "  for (k in 1:K){\n",
        "    pam_fit <- pam(dist, diss = T, k = k)\n",
        "    obj.fun[k] <- calc.wss(pam_fit)\n",
        "  }\n",
        "  return(obj.fun)\n",
        "}\n",
        "\n",
        "step2 <- function(dist, data, m, B, K){\n",
        "  # step 2\n",
        "  art.obj.fun <- matrix(list(), nrow = B, ncol = K)\n",
        "  for (b in 1:B){\n",
        "    artificial.data <- create.artificial.data(m, data)\n",
        "    for(k in 1:K){\n",
        "      pam_fit <- pam(as.dist(gower.dist(artificial.data)), diss = T, k = k)\n",
        "      art.obj.fun[[b, k]] <- calc.wss(pam_fit)#pam_fit$objective[1]\n",
        "    }\n",
        "  }\n",
        "  return(art.obj.fun)\n",
        "}\n",
        "\n",
        "step3 <- function(B, K, obj.fun, art.obj.fun){\n",
        "  #step 3\n",
        "  gap <- c()\n",
        "  for(k in 1:K){\n",
        "    sum.obj.fun <- 0\n",
        "    for (b in 1:B){\n",
        "      sum.obj.fun <- sum.obj.fun + log(art.obj.fun[[b, k]])\n",
        "    }\n",
        "    gap[k] <- 1/B * sum.obj.fun - log(obj.fun[k])\n",
        "  }\n",
        "  #plot(1:K, gap,\n",
        "  #   xlab = \"Number of clusters\",\n",
        "  #   ylab = \"Silhouette Width\",\n",
        "  #   main = \"Gap Plot\")\n",
        "  #lines(1:K, gap)\n",
        "  return(gap)\n",
        "}\n",
        "\n",
        "step4 <- function(B, K, art.obj.fun, gap){\n",
        "  for(k in 1:(K-1)){\n",
        "    sd <- 0\n",
        "    mean <- 0\n",
        "    for(b in 1:B) {\n",
        "      mean <- mean + log(art.obj.fun[[b, k + 1]])\n",
        "    }\n",
        "    mean <- 1/B * mean\n",
        "    intern <- 0\n",
        "    for(b in 1:B){\n",
        "      intern <- intern + ((log(art.obj.fun[[b, k + 1]]) - mean ) ** 2)\n",
        "    }\n",
        "    sd <- sqrt( 1/B * intern )\n",
        "    s <- sqrt(1 + 1/B) * sd\n",
        "    if (gap[k] >= gap[k+1] - s) {\n",
        "      return(k)\n",
        "    }\n",
        "  }\n",
        "  return(K)\n",
        "}\n",
        "# GAP function\n",
        "pam.GAP <- function(dist, data, m, B, K){\n",
        "  step1(dist = dist, K = K) -> obj.fun\n",
        "  step2(dist = dist, data = data, m = m, B = B, K = K) -> art.obj.fun\n",
        "  step3(B = B, K = K, obj.fun = obj.fun, art.obj.fun = art.obj.fun) -> gap\n",
        "  step4(B = B, K = K, art.obj.fun = art.obj.fun, gap = gap) -> k\n",
        "  return(k)\n",
        "}\n",
        "\n",
        "checkBestN <- function(){\n",
        "  # GAP\n",
        "  #pam.gap <- pam.GAP(dist = dist, data = insurance.scaled, m = 500, B = 50, K = 6)\n",
        "  print(\"starting --> pam\")\n",
        "  pam.clvalid <- myClValid::myClValid.cat(insurance.scaled, nClust = 2:6, clMethods = \"pam\",\n",
        "                                    validation = c(\"internal\", \"stability\"),\n",
        "                                    maxitems = nrow(insurance.scaled) + 1)\n",
        "  print(\"starting --> hierarchical ward.D\")\n",
        "  h.ward.D <- myClValid::optimalScores(myClValid::myClValid.cat(insurance.scaled, nClust = 2:6, clMethods = \"hierarchical\",\n",
        "                                    validation = c(\"internal\", \"stability\"),\n",
        "                                    maxitems = nrow(insurance.scaled) + 1,\n",
        "                                    method = \"ward.D\"))\n",
        "  print(\"starting --> hierarchical ward.D2\")\n",
        "  h.ward.D2 <- myClValid::optimalScores(myClValid::myClValid.cat(insurance.scaled, nClust = 2:6, clMethods = \"hierarchical\",\n",
        "                                    validation = c(\"internal\", \"stability\"),\n",
        "                                    maxitems = nrow(insurance.scaled) + 1,\n",
        "                                    method = \"ward.D2\"))\n",
        "  print(\"starting --> hierarchical single\")\n",
        "  h.single <- myClValid::optimalScores(myClValid::myClValid.cat(insurance.scaled, nClust = 2:6, clMethods = \"hierarchical\",\n",
        "                                    validation = c(\"internal\", \"stability\"),\n",
        "                                    maxitems = nrow(insurance.scaled) + 1,\n",
        "                                    method = \"single\"))\n",
        "  print(\"starting --> hierarchical complete\")\n",
        "  h.complete <-myClValid::optimalScores(myClValid::myClValid.cat(insurance.scaled, nClust = 2:6, clMethods = \"hierarchical\",\n",
        "                                    validation = c(\"internal\", \"stability\"),\n",
        "                                    maxitems = nrow(insurance.scaled) + 1,\n",
        "                                    method = \"complete\"))\n",
        "  print(\"starting --> hierarchical complete\")\n",
        "  h.average <- myClValid::optimalScores(myClValid::myClValid.cat(insurance.scaled, nClust = 2:6, clMethods = \"hierarchical\",\n",
        "                                    validation = c(\"internal\", \"stability\"),\n",
        "                                    maxitems = nrow(insurance.scaled) + 1,\n",
        "                                    method = \"average\"))\n",
        "  print(\"end\")\n",
        "\n",
        "  optimals <- myClValid::optimalScores(pam.clvalid)\n",
        "  optimals <- rbind(optimals, h.ward.D,h.ward.D2,h.single,h.complete,h.average)\n",
        "  return(optimals[,-1])\n",
        "}\n",
        "\n",
        "# elbow\n",
        "#pam.elbow(dist = dist, K = 6)\n",
        "#proviamo <- checkBestN()\n",
        "#print(proviamo)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ZCQO83Y6CIy"
      },
      "source": [
        " print(\"starting --> pam\")\n",
        "  pam.clvalid <- myClValid::myClValid.cat(insurance.scaled, nClust = 2:6, clMethods = \"pam\",\n",
        "                                    validation = c(\"internal\", \"stability\"),\n",
        "                                    maxitems = nrow(insurance.scaled) + 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BUU-v4Lr6E_V"
      },
      "source": [
        " print(\"starting --> hierarchical ward.D\")\n",
        "  h.ward.D <- myClValid::optimalScores(myClValid::myClValid.cat(insurance.scaled, nClust = 2:6, clMethods = \"hierarchical\",\n",
        "                                    validation = c(\"internal\", \"stability\"),\n",
        "                                    maxitems = nrow(insurance.scaled) + 1,\n",
        "                                    method = \"ward.D\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zKjvpBTm6IQe"
      },
      "source": [
        " print(\"starting --> hierarchical ward.D2\")\n",
        "  h.ward.D2 <- myClValid::optimalScores(myClValid::myClValid.cat(insurance.scaled, nClust = 2:6, clMethods = \"hierarchical\",\n",
        "                                    validation = c(\"internal\", \"stability\"),\n",
        "                                    maxitems = nrow(insurance.scaled) + 1,\n",
        "                                    method = \"ward.D2\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AI_UPCiK6KxH"
      },
      "source": [
        " print(\"starting --> hierarchical single\")\n",
        "  h.single <- myClValid::optimalScores(myClValid::myClValid.cat(insurance.scaled, nClust = 2:6, clMethods = \"hierarchical\",\n",
        "                                    validation = c(\"internal\", \"stability\"),\n",
        "                                    maxitems = nrow(insurance.scaled) + 1,\n",
        "                                    method = \"single\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2mDhvypV6Nc-"
      },
      "source": [
        " print(\"starting --> hierarchical complete\")\n",
        "  h.complete <-myClValid::optimalScores(myClValid::myClValid.cat(insurance.scaled, nClust = 2:6, clMethods = \"hierarchical\",\n",
        "                                    validation = c(\"internal\", \"stability\"),\n",
        "                                    maxitems = nrow(insurance.scaled) + 1,\n",
        "                                    method = \"complete\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "htZKbAd26QR7"
      },
      "source": [
        "  print(\"starting --> hierarchical average\")\n",
        "  h.average <- myClValid::optimalScores(myClValid::myClValid.cat(insurance.scaled, nClust = 2:6, clMethods = \"hierarchical\",\n",
        "                                    validation = c(\"internal\", \"stability\"),\n",
        "                                    maxitems = nrow(insurance.scaled) + 1,\n",
        "                                    method = \"average\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZfKJZkS96ZRz"
      },
      "source": [
        "optimals <- myClValid::optimalScores(pam.clvalid)\n",
        "optimals <- rbind(optimals, h.ward.D,h.ward.D2,h.single,h.complete,h.average)\n",
        "print(optimals)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8oaf81DEoRbm"
      },
      "source": [
        "Now that we have defined the functions we will check the optimal number of clusters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aoIN0FrAqBTh"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z2rKx-tCf2Li"
      },
      "source": [
        "cazzo()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3XoigCUpgDeA"
      },
      "source": [
        "#pam.silh <- \n",
        "#summary(pam.silh)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IMwSj5pyQef2"
      },
      "source": [
        "\n",
        "summary(pam.dunn)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5cOUkKr1_fFd"
      },
      "source": [
        "#cazzo()\n",
        "#pam.GAP(dist = dist, data = insurance.scaled, m = 11, B = 1, K = 10)\n",
        "clmethods <- c(\"hierarchical\", \"pam\")\n",
        "validations <- c(\"internal\", \"stability\")\n",
        "methods <- c(\"ward.D\", \"single\", \"complete\", \"average\")\n",
        "clvalid <- myClValid::myClValid.cat(insurance.scaled, nClust = 2:6, clMethods = clmethods,\n",
        "                                    validation = validations,\n",
        "                                    maxitems = nrow(insurance.scaled),\n",
        "                                    method = methods)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r9mPz1hwj8g5"
      },
      "source": [
        "#pam <- pam(dist, diss = T, 4)\n",
        "#install.packages(\"clValid\")\n",
        "#library(fpc)\n",
        "#cluster.stats(dist, pam(dist, diss = T, 3)$cluster)\n",
        "#library(clValid)\n",
        "#trace(\"clValid\", edit = T)\n",
        "#install.packages(c(\"devtools\", \"StatMatch\", \"data.table\"))\n",
        "#library(devtools)\n",
        "#library(data.table)\n",
        "#install_github(\"biagio-incardona/myClValid\", force = T)\n",
        "#library(clValid)\n",
        "clmethods <- c(\"hierarchical\", \"pam\")\n",
        "insurance.data <- read.csv(\"Healt_Insurance.csv\")\n",
        "\n",
        "insurance.data$Driving_License <- as.factor(ifelse(insurance.data$Driving_License == 1, \"Yes\", \"No\"))\n",
        "insurance.data$Previously_Insured <- as.factor(ifelse(insurance.data$Previously_Insured == 1, \"Yes\", \"No\"))\n",
        "insurance.data$Gender <- as.factor(insurance.data$Gender)\n",
        "insurance.data$Vehicle_Damage <- as.factor(insurance.data$Vehicle_Damage)\n",
        "insurance.data$Vehicle_Age <- as.factor(insurance.data$Vehicle_Age)\n",
        "\n",
        "clmethods <- c(\"hierarchical\", \"pam\")\n",
        "validations <- c(\"internal\", \"stability\")\n",
        "methods <- c(\"ward.D\")\n",
        "clvalid1 <- myClValid::myClValid.cat(insurance.data[1:20,], nClust = 2:6, clMethods = clmethods,\n",
        "                                    validation = validations,\n",
        "                                    maxitems = 5001,\n",
        "                                    method = \"ward.D\")                                                                                                                                           \n",
        "myClValid::optimalScores(clvalid1)\n",
        "#myClValid::optimalScores(clvalid2)\n",
        "#myClValid::optimalScores(clvalid3)\n",
        "#myClValid::optimalScores(clvalid4)\n",
        "#myClValid::optimalScores(clvalid5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VNq7HQqpMEqT"
      },
      "source": [
        "asd <- hclust(d = dist)\n",
        "plot(asd)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lfCFOOKLBxGW"
      },
      "source": [
        "insurance.data <- read.csv(\"Healt_Insurance.csv\")\n",
        "insurance.data$Driving_License <- as.factor(ifelse(insurance.data$Driving_License == 1, \"Yes\", \"No\"))\n",
        "insurance.data$Previously_Insured <- as.factor(ifelse(insurance.data$Previously_Insured == 1, \"Yes\", \"No\"))\n",
        "insurance.data$Gender <- as.factor(insurance.data$Gender)\n",
        "insurance.data$Vehicle_Damage <- as.factor(insurance.data$Vehicle_Damage)\n",
        "insurance.data$Vehicle_Age <- as.factor(insurance.data$Vehicle_Age)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IRCsnS8w8JyU"
      },
      "source": [
        "#x <- daisy(insurance.data[1:5000,-12], metric=\"gower\")\n",
        "summary(intern)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FLbvGk85Fbuw"
      },
      "source": [
        "gower gower.dist(insurance.data[1:5000,-12])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gJyssjy1j8g5"
      },
      "source": [
        "### 7.3 Cluster validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FgbvRJFOj8g5"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_FVeO-Rxj8g5"
      },
      "source": [
        "#library(ggplot2, warn.conflicts = F, quietly = T)\n",
        "#library(dplyr, warn.conflicts = F, quietly = T)\n",
        "#library(Hmisc, warn.conflicts = F, quietly = T)\n",
        "#library(data.table, warn.conflicts = F, quietly = T)\n",
        "#library(corrplot, warn.conflicts = F, quietly = T)\n",
        "#library(factoextra, warn.conflicts = F, quietly = T)\n",
        "#library(plot3D, warn.conflicts = F, quietly = T)\n",
        "#library(ggridges, warn.conflicts = F, quietly = T)\n",
        "#library(tidyr, warn.conflicts = F, quietly = T)\n",
        "#library(clustertend, warn.conflicts = F, quietly = T)\n",
        "\n",
        "#709\n",
        "#max_len <- -1\n",
        "#seed <- 0\n",
        "#insurance.data.orig <- read.csv('Healt_Insurance.csv')\n",
        "\n",
        "#set.seed(12999)\n",
        "\n",
        "#rows <- sample(1:nrow(insurance.data.orig), 5000)\n",
        "\n",
        "#insurance.data <- insurance.data.orig[rows,]\n",
        "#insurance.data$id <- NULL\n",
        "#rownames(insurance.data) <- 1:nrow(insurance.data)\n",
        "# ordering the factors\n",
        "#insurance.data$Vehicle_Age <- ordered(insurance.data$Vehicle_Age, levels = c('< 1 Year','1-2 Year', '> 2 Years'))\n",
        "#insurance.data$Vehicle_Age <- as.integer(insurance.data$Vehicle_Age)\n",
        "# check if the variable becomes ordered\n",
        "#insurance.data$Vintage <- insurance.data$Vintage/365\n",
        "# transform the dataset into a data.table\n",
        "#data.table <- as.data.table(insurance.data)\n",
        "\n",
        "# apply one hot encoding to the transformed dataset\n",
        "#insurance.ohe.datatable <- mltools :: one_hot(as.data.table(insurance.data))\n",
        "\n",
        "\n",
        "# transform the encoded data table into a dataset\n",
        "#insurance.ohe <- as.data.frame(insurance.ohe.datatable)\n",
        "#insurance.scaled <- as.data.frame(scale(insurance.ohe,center = F, scale = T))\n",
        "# calculate pca\n",
        "#pca <- prcomp(insurance.scaled, scale. = F, center = F)\n",
        "\n",
        "# show some informations\n",
        "#summary.pca <- summary(pca)\n",
        "\n",
        "# get the first 3 Principal components\n",
        "#PC1 <- pca$x[,1]\n",
        "#PC2 <- pca$x[,2]\n",
        "#PC3 <- pca$x[,3]\n",
        "\n",
        "# plot each Principal comonents vs all others\n",
        "#pairs(as.data.frame(cbind(PC1, PC2, PC3)))\n",
        "#scatter3D(PC1, PC2, PC3, phi = 0, bty =\"g\",\n",
        "#                  xlab = \"PC1\", ylab = \"PC2\", zlab = \"PC3\")\n",
        "\n",
        "#\n",
        "\n",
        "# calculate the lower bound\n",
        "#lower_bound <- median(pca$x[,1]) - 3 * mad(pca$x[,1])\n",
        "\n",
        "# calculate the upper bound\n",
        "#upper_bound <- median(pca$x[,1]) + 3 * mad(pca$x[,1])\n",
        "\n",
        "# get the outliers indexes\n",
        "#left_outlier_ind <- which(pca$x[,1] < lower_bound)\n",
        "#right_outlier_ind <- which(pca$x[,1] > upper_bound)\n",
        "\n",
        "# creating a copy of the data\n",
        "#hampel.data <- pca$x[,1:3]\n",
        "#hampel.data <- as.data.frame(hampel.data)\n",
        "\n",
        "# set the sampler as outlier\n",
        "#hampel.data$is.outlier <- 1:nrow(hampel.data)\n",
        "\n",
        "#hampel.data$is.outlier[-left_outlier_ind] = 1 #no outliers\n",
        "#hampel.data$is.outlier[left_outlier_ind] = 2 # < lower_bound\n",
        "#hampel.data$is.outlier[right_outlier_ind] = 3 # > upper_bound\n",
        "\n",
        "# plot the new data\n",
        "#colVar <- sapply(hampel.data$is.outlier, function(a){ifelse(a == 1,'red', ifelse(a == 2,'green', 'blue'))})\n",
        "#colVar <- factor(colVar,levels=c('red','green','blue'))\n",
        "\n",
        "#scatter3D(x=PC1,y=PC2,z=PC3, phi = 0, bty =\"g\",\n",
        "#          xlab = \"PC1\", ylab = \"PC2\", zlab = \"PC3\",\n",
        "#          colvar=as.integer(colVar),\n",
        "#          colkey=list(at=c(1,2,3),side=4),\n",
        "#          col=as.character(levels(colVar)))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mh-22i74j8g5"
      },
      "source": [
        "#length(left_outlier_ind) + length(right_outlier_ind)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}